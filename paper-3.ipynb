{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# 1. Bulid the datset of the unidentified workers"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# 2. Define the file dir of datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-16T12:10:56.743544Z","iopub.status.busy":"2023-04-16T12:10:56.743178Z","iopub.status.idle":"2023-04-16T12:10:56.749439Z","shell.execute_reply":"2023-04-16T12:10:56.748125Z","shell.execute_reply.started":"2023-04-16T12:10:56.743508Z"},"trusted":true},"outputs":[],"source":["# GT_FILE = 'data/AQ_GTD.csv'\n","# UN_FILE = 'data/AQ_unkown.csv'\n","GT_FILE = 'data/AEP_GTD.csv'\n","UN_FILE = 'data/AEP_unkown.csv'"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# 3. OC-SVM sensitivity, specificity，accuracy，F1-score"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-04-16T12:10:56.763478Z","iopub.status.busy":"2023-04-16T12:10:56.762563Z","iopub.status.idle":"2023-04-16T12:10:57.961306Z","shell.execute_reply":"2023-04-16T12:10:57.959970Z","shell.execute_reply.started":"2023-04-16T12:10:56.763434Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Sensitivity: [0.975, 0.95, 0.95, 0.975, 0.95, 0.95, 0.95, 0.925, 0.975, 0.975]\n","Specificity: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n","Accuracy: [0.99, 0.98, 0.98, 0.99, 0.98, 0.98, 0.98, 0.97, 0.99, 0.99]\n","F1-score: [0.9873417721518987, 0.9743589743589743, 0.9743589743589743, 0.9873417721518987, 0.9743589743589743, 0.9743589743589743, 0.9743589743589743, 0.961038961038961, 0.9873417721518987, 0.9873417721518987]\n"]}],"source":["import numpy as np\n","import pandas as pd\n","from sklearn.svm import OneClassSVM\n","from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n","from sklearn.preprocessing import normalize\n","import pandas as pd\n","from sklearn.preprocessing import StandardScaler\n","\n","# 加载 CSV 文件\n","data = pd.read_csv(GT_FILE)\n","unidentified_data = pd.read_csv(UN_FILE)\n","labels = unidentified_data['Label'].values\n","unidentified_data = unidentified_data.drop(columns=['Label'])\n","\n","scaler = StandardScaler()\n","data_scaled = scaler.fit_transform(data)\n","unidentified_data_scaled = scaler.transform(unidentified_data)\n","# Convert the standardized data arrays back to dataframes\n","data = pd.DataFrame(data_scaled,columns=data.columns)\n","unidentified_data = pd.DataFrame(unidentified_data_scaled,columns=unidentified_data.columns)\n","\n","n_columns = data.shape[1]\n","n_columns_1 = unidentified_data.shape[1]\n","sensitivity_list = []\n","specificity_list = []\n","accuracy_list = []\n","f1_score_list = []\n","# 每两列进行异常检测\n","for i in range(1, n_columns, 2):\n","    X = data.iloc[:, i:i + 2].values\n","#     使用 One-Class SVM 进行异常检测\n","    clf = OneClassSVM(nu=0.01, kernel=\"rbf\", gamma=0.05)\n","    clf.fit(X)\n","    X_2 = unidentified_data.iloc[:, i:i + 2].values\n","    y_pred = clf.predict(X_2)\n","    # 将 One-Class SVM 的输出转换为与原始标签相同的格式（1 表示正常，0 表示异常）\n","    y_pred = np.where(y_pred == 1, 1, 0)\n","    # 计算混淆矩阵\n","    tn, fp, fn, tp = confusion_matrix(labels, y_pred).ravel()\n","    # 计算灵敏度（召回率）、特异度、正确率和 F1-score\n","    sensitivity = tp / (tp + fn)\n","    specificity = tn / (tn + fp)\n","    accuracy = accuracy_score(labels, y_pred)\n","    f1 = f1_score(labels, y_pred)\n","    # 保存每一次的结果\n","    sensitivity_list.append(sensitivity)\n","    specificity_list.append(specificity)\n","    accuracy_list.append(accuracy)\n","    f1_score_list.append(f1)\n","print(\"Sensitivity:\", sensitivity_list)\n","print(\"Specificity:\", specificity_list)\n","print(\"Accuracy:\", accuracy_list)\n","print(\"F1-score:\", f1_score_list)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# 4. MTI sensitivity, specificity，accuracy，F1-score"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-04-16T12:10:57.964220Z","iopub.status.busy":"2023-04-16T12:10:57.963826Z","iopub.status.idle":"2023-04-16T12:10:58.052449Z","shell.execute_reply":"2023-04-16T12:10:58.050860Z","shell.execute_reply.started":"2023-04-16T12:10:57.964182Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Column: ID Sensitivity: 0.00 Specificity: 0.93 Accuracy: 0.56 F1-score: 0.00\n","----------------------------------------------------------------------------------------------------\n","Column: L1_1 Sensitivity: 0.55 Specificity: 0.78 Accuracy: 0.69 F1-score: 0.59\n","----------------------------------------------------------------------------------------------------\n","Column: L1_2 Sensitivity: 0.00 Specificity: 1.00 Accuracy: 0.60 F1-score: 0.00\n","----------------------------------------------------------------------------------------------------\n","Column: L2_1 Sensitivity: 0.38 Specificity: 0.90 Accuracy: 0.69 F1-score: 0.49\n","----------------------------------------------------------------------------------------------------\n","Column: L2_2 Sensitivity: 0.00 Specificity: 1.00 Accuracy: 0.60 F1-score: 0.00\n","----------------------------------------------------------------------------------------------------\n","Column: L3_1 Sensitivity: 1.00 Specificity: 0.85 Accuracy: 0.91 F1-score: 0.90\n","----------------------------------------------------------------------------------------------------\n","Column: L3_2 Sensitivity: 0.00 Specificity: 1.00 Accuracy: 0.60 F1-score: 0.00\n","----------------------------------------------------------------------------------------------------\n","Column: L4_1 Sensitivity: 0.78 Specificity: 0.90 Accuracy: 0.85 F1-score: 0.81\n","----------------------------------------------------------------------------------------------------\n","Column: L4_2 Sensitivity: 0.00 Specificity: 1.00 Accuracy: 0.60 F1-score: 0.00\n","----------------------------------------------------------------------------------------------------\n","Column: L5_1 Sensitivity: 0.82 Specificity: 1.00 Accuracy: 0.93 F1-score: 0.90\n","----------------------------------------------------------------------------------------------------\n","Column: L5_2 Sensitivity: 0.00 Specificity: 1.00 Accuracy: 0.60 F1-score: 0.00\n","----------------------------------------------------------------------------------------------------\n","Column: L6_1 Sensitivity: 0.00 Specificity: 0.65 Accuracy: 0.39 F1-score: 0.00\n","----------------------------------------------------------------------------------------------------\n","Column: L6_2 Sensitivity: 0.00 Specificity: 1.00 Accuracy: 0.60 F1-score: 0.00\n","----------------------------------------------------------------------------------------------------\n","Column: L7_1 Sensitivity: 1.00 Specificity: 0.95 Accuracy: 0.97 F1-score: 0.96\n","----------------------------------------------------------------------------------------------------\n","Column: L7_2 Sensitivity: 0.00 Specificity: 1.00 Accuracy: 0.60 F1-score: 0.00\n","----------------------------------------------------------------------------------------------------\n","Column: L8_1 Sensitivity: 0.65 Specificity: 1.00 Accuracy: 0.86 F1-score: 0.79\n","----------------------------------------------------------------------------------------------------\n","Column: L8_2 Sensitivity: 0.00 Specificity: 1.00 Accuracy: 0.60 F1-score: 0.00\n","----------------------------------------------------------------------------------------------------\n","Column: L9_1 Sensitivity: 1.00 Specificity: 0.87 Accuracy: 0.92 F1-score: 0.91\n","----------------------------------------------------------------------------------------------------\n","Column: L9_2 Sensitivity: 0.00 Specificity: 1.00 Accuracy: 0.60 F1-score: 0.00\n","----------------------------------------------------------------------------------------------------\n","Column: L10_1 Sensitivity: 0.00 Specificity: 1.00 Accuracy: 0.60 F1-score: 0.00\n","----------------------------------------------------------------------------------------------------\n","Column: L10_2 Sensitivity: 0.00 Specificity: 1.00 Accuracy: 0.60 F1-score: 0.00\n","----------------------------------------------------------------------------------------------------\n"]}],"source":["import pandas as pd\n","from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n","def main():\n","    # 读取CSV文件\n","    df = pd.read_csv(UN_FILE)\n","    # df = df.iloc[:].drop('Unnamed: 0',axis=1)\n","    # 转换为数值类型\n","    df = df.apply(pd.to_numeric, errors='coerce')\n","    # 计算每一列的均值\n","    mean_values = df.mean()\n","    # 设置阈值\n","    threshold = 2\n","    # 遍历所有列并计算灵敏度、特异度、正确率和F1-score\n","    for column in df.columns:\n","        if column == 'Label':\n","            continue\n","        # 使用均值和阈值作为分类器\n","        df['predicted'] = df[column].apply(lambda x: 1 if mean_values[column] - threshold <= x <= mean_values[column] + threshold else 0)\n","        # 计算混淆矩阵\n","        cm = confusion_matrix(df['Label'], df['predicted'])\n","        # 计算灵敏度、特异度、正确率和F1-score\n","        sensitivity = cm[1, 1] / (cm[1, 1] + cm[1, 0])\n","        specificity = cm[0, 0] / (cm[0, 0] + cm[0, 1])\n","        accuracy = accuracy_score(df['Label'], df['predicted'])\n","        f1 = f1_score(df['Label'], df['predicted'])\n","        print(f\"Column: {column}\",f\"Sensitivity: {sensitivity:.2f}\",f\"Specificity: {specificity:.2f}\"\n","              ,f\"Accuracy: {accuracy:.2f}\",f\"F1-score: {f1:.2f}\")\n","        print(\"-\"*100)\n","if __name__ == \"__main__\":\n","    main()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# 5. MVI sensitivity, specificity，accuracy，F1-score"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-04-16T12:10:58.054505Z","iopub.status.busy":"2023-04-16T12:10:58.054009Z","iopub.status.idle":"2023-04-16T12:10:58.139337Z","shell.execute_reply":"2023-04-16T12:10:58.137858Z","shell.execute_reply.started":"2023-04-16T12:10:58.054456Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Column: ID Sensitivity: 0.03 Specificity: 1.00 Accuracy: 0.61 F1-score: 0.05\n","----------------------------------------------------------------------------------------------------\n","Column: L1_1 Sensitivity: 0.17 Specificity: 1.00 Accuracy: 0.67 F1-score: 0.30\n","----------------------------------------------------------------------------------------------------\n","Column: L1_2 Sensitivity: 0.10 Specificity: 1.00 Accuracy: 0.64 F1-score: 0.18\n","----------------------------------------------------------------------------------------------------\n","Column: L2_1 Sensitivity: 0.23 Specificity: 1.00 Accuracy: 0.69 F1-score: 0.37\n","----------------------------------------------------------------------------------------------------\n","Column: L2_2 Sensitivity: 0.10 Specificity: 1.00 Accuracy: 0.64 F1-score: 0.18\n","----------------------------------------------------------------------------------------------------\n","Column: L3_1 Sensitivity: 0.25 Specificity: 1.00 Accuracy: 0.70 F1-score: 0.40\n","----------------------------------------------------------------------------------------------------\n","Column: L3_2 Sensitivity: 0.10 Specificity: 1.00 Accuracy: 0.64 F1-score: 0.18\n","----------------------------------------------------------------------------------------------------\n","Column: L4_1 Sensitivity: 0.23 Specificity: 1.00 Accuracy: 0.69 F1-score: 0.37\n","----------------------------------------------------------------------------------------------------\n","Column: L4_2 Sensitivity: 0.05 Specificity: 1.00 Accuracy: 0.62 F1-score: 0.10\n","----------------------------------------------------------------------------------------------------\n","Column: L5_1 Sensitivity: 0.20 Specificity: 1.00 Accuracy: 0.68 F1-score: 0.33\n","----------------------------------------------------------------------------------------------------\n","Column: L5_2 Sensitivity: 0.10 Specificity: 1.00 Accuracy: 0.64 F1-score: 0.18\n","----------------------------------------------------------------------------------------------------\n","Column: L6_1 Sensitivity: 0.07 Specificity: 1.00 Accuracy: 0.63 F1-score: 0.14\n","----------------------------------------------------------------------------------------------------\n","Column: L6_2 Sensitivity: 0.05 Specificity: 1.00 Accuracy: 0.62 F1-score: 0.10\n","----------------------------------------------------------------------------------------------------\n","Column: L7_1 Sensitivity: 0.00 Specificity: 0.77 Accuracy: 0.46 F1-score: 0.00\n","----------------------------------------------------------------------------------------------------\n","Column: L7_2 Sensitivity: 0.00 Specificity: 0.77 Accuracy: 0.46 F1-score: 0.00\n","----------------------------------------------------------------------------------------------------\n","Column: L8_1 Sensitivity: 0.00 Specificity: 0.77 Accuracy: 0.46 F1-score: 0.00\n","----------------------------------------------------------------------------------------------------\n","Column: L8_2 Sensitivity: 0.00 Specificity: 0.77 Accuracy: 0.46 F1-score: 0.00\n","----------------------------------------------------------------------------------------------------\n","Column: L9_1 Sensitivity: 0.62 Specificity: 1.00 Accuracy: 0.85 F1-score: 0.77\n","----------------------------------------------------------------------------------------------------\n","Column: L9_2 Sensitivity: 0.00 Specificity: 0.77 Accuracy: 0.46 F1-score: 0.00\n","----------------------------------------------------------------------------------------------------\n","Column: L10_1 Sensitivity: 0.00 Specificity: 0.77 Accuracy: 0.46 F1-score: 0.00\n","----------------------------------------------------------------------------------------------------\n","Column: L10_2 Sensitivity: 0.00 Specificity: 0.77 Accuracy: 0.46 F1-score: 0.00\n","----------------------------------------------------------------------------------------------------\n"]}],"source":["import pandas as pd\n","from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n","def MV(column):\n","    value_counts = column.value_counts()\n","    majority_value = value_counts.idxmax()\n","    return column.apply(lambda x: 1 if x == majority_value else 0)\n","def main():\n","    # 读取CSV文件\n","    df = pd.read_csv(UN_FILE)\n","    # df = df.iloc[:].drop('Unnamed: 0',axis=1)\n","    # 转换为数值类型\n","    df = df.apply(pd.to_numeric, errors='coerce')\n","    # 遍历所有列并计算灵敏度、特异度、正确率和F1-score\n","    for column in df.columns:\n","        if column == 'Label':\n","            continue\n","        # 使用MV推理方法作为分类器\n","        df['predicted'] = MV(df[column])\n","        # 计算混淆矩阵\n","        cm = confusion_matrix(df['Label'], df['predicted'])\n","        # 计算灵敏度、特异度、正确率和F1-score\n","        sensitivity = cm[1, 1] / (cm[1, 1] + cm[1, 0])\n","        specificity = cm[0, 0] / (cm[0, 0] + cm[0, 1])\n","        accuracy = accuracy_score(df['Label'], df['predicted'])\n","        f1 = f1_score(df['Label'], df['predicted'])\n","        print(f\"Column: {column}\",f\"Sensitivity: {sensitivity:.2f}\",f\"Specificity: {specificity:.2f}\",\n","              f\"Accuracy: {accuracy:.2f}\",f\"F1-score: {f1:.2f}\")\n","        print(\"-\"*100)\n","if __name__ == \"__main__\":\n","    main()\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# 6. WTI sensitivity, specificity，accuracy，F1-score"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-04-16T12:10:58.143981Z","iopub.status.busy":"2023-04-16T12:10:58.142888Z","iopub.status.idle":"2023-04-16T12:10:58.307330Z","shell.execute_reply":"2023-04-16T12:10:58.306260Z","shell.execute_reply.started":"2023-04-16T12:10:58.143926Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Column: ID Sensitivity: 0.00 Specificity: 0.17 Accuracy: 0.10 F1-score: 0.00\n","----------------------------------------------------------------------------------------------------\n","Column: L1_1 Sensitivity: 0.97 Specificity: 0.50 Accuracy: 0.69 F1-score: 0.72\n","----------------------------------------------------------------------------------------------------\n","Column: L1_2 Sensitivity: 1.00 Specificity: 0.90 Accuracy: 0.94 F1-score: 0.93\n","----------------------------------------------------------------------------------------------------\n","Column: L2_1 Sensitivity: 1.00 Specificity: 0.57 Accuracy: 0.74 F1-score: 0.75\n","----------------------------------------------------------------------------------------------------\n","Column: L2_2 Sensitivity: 1.00 Specificity: 0.88 Accuracy: 0.93 F1-score: 0.92\n","----------------------------------------------------------------------------------------------------\n","Column: L3_1 Sensitivity: 0.70 Specificity: 0.52 Accuracy: 0.59 F1-score: 0.58\n","----------------------------------------------------------------------------------------------------\n","Column: L3_2 Sensitivity: 1.00 Specificity: 0.87 Accuracy: 0.92 F1-score: 0.91\n","----------------------------------------------------------------------------------------------------\n","Column: L4_1 Sensitivity: 0.78 Specificity: 0.57 Accuracy: 0.65 F1-score: 0.64\n","----------------------------------------------------------------------------------------------------\n","Column: L4_2 Sensitivity: 1.00 Specificity: 0.90 Accuracy: 0.94 F1-score: 0.93\n","----------------------------------------------------------------------------------------------------\n","Column: L5_1 Sensitivity: 0.78 Specificity: 0.52 Accuracy: 0.62 F1-score: 0.62\n","----------------------------------------------------------------------------------------------------\n","Column: L5_2 Sensitivity: 1.00 Specificity: 0.85 Accuracy: 0.91 F1-score: 0.90\n","----------------------------------------------------------------------------------------------------\n","Column: L6_1 Sensitivity: 0.00 Specificity: 0.20 Accuracy: 0.12 F1-score: 0.00\n","----------------------------------------------------------------------------------------------------\n","Column: L6_2 Sensitivity: 1.00 Specificity: 0.88 Accuracy: 0.93 F1-score: 0.92\n","----------------------------------------------------------------------------------------------------\n","Column: L7_1 Sensitivity: 0.88 Specificity: 0.63 Accuracy: 0.73 F1-score: 0.72\n","----------------------------------------------------------------------------------------------------\n","Column: L7_2 Sensitivity: 1.00 Specificity: 0.93 Accuracy: 0.96 F1-score: 0.95\n","----------------------------------------------------------------------------------------------------\n","Column: L8_1 Sensitivity: 0.70 Specificity: 0.60 Accuracy: 0.64 F1-score: 0.61\n","----------------------------------------------------------------------------------------------------\n","Column: L8_2 Sensitivity: 1.00 Specificity: 0.98 Accuracy: 0.99 F1-score: 0.99\n","----------------------------------------------------------------------------------------------------\n","Column: L9_1 Sensitivity: 0.30 Specificity: 0.70 Accuracy: 0.54 F1-score: 0.34\n","----------------------------------------------------------------------------------------------------\n","Column: L9_2 Sensitivity: 1.00 Specificity: 0.97 Accuracy: 0.98 F1-score: 0.98\n","----------------------------------------------------------------------------------------------------\n","Column: L10_1 Sensitivity: 0.93 Specificity: 0.78 Accuracy: 0.84 F1-score: 0.82\n","----------------------------------------------------------------------------------------------------\n","Column: L10_2 Sensitivity: 1.00 Specificity: 0.98 Accuracy: 0.99 F1-score: 0.99\n","----------------------------------------------------------------------------------------------------\n"]}],"source":["import numpy as np\n","import pandas as pd\n","from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n","def WTI(column, threshold=1e-4):\n","    def calculate_weighted_mean(column, weights):\n","        return np.average(column, weights=weights)\n","    def calculate_weights(column, mean):\n","        distances = np.abs(column - mean)\n","        return np.square(1 / (distances + 1e-8))\n","    prev_mean = column.mean()\n","    while True:\n","        weights = calculate_weights(column, prev_mean)\n","        new_mean = calculate_weighted_mean(column, weights)\n","        if np.abs(new_mean - prev_mean) < threshold:\n","            break\n","        prev_mean = new_mean\n","    return column.apply(lambda x: 1 if x >= new_mean else 0)\n","def main():\n","    # 读取CSV文件\n","    df = pd.read_csv(UN_FILE)\n","    # df = df.iloc[:].drop('Unnamed: 0',axis=1)\n","    # 转换为数值类型\n","    df = df.apply(pd.to_numeric, errors='coerce')\n","    # 遍历所有列并计算灵敏度、特异度、正确率和F1-score\n","    for column in df.columns:\n","        if column == 'Label':\n","            continue\n","        # 使用WTI推理方法作为分类器\n","        df['predicted'] = WTI(df[column])\n","        # 计算混淆矩阵\n","        cm = confusion_matrix(df['Label'], df['predicted'])\n","        # 计算灵敏度、特异度、正确率和F1-score\n","        sensitivity = cm[1, 1] / (cm[1, 1] + cm[1, 0])\n","        specificity = cm[0, 0] / (cm[0, 0] + cm[0, 1])\n","        accuracy = accuracy_score(df['Label'], df['predicted'])\n","        f1 = f1_score(df['Label'], df['predicted'])\n","        print(f\"Column: {column}\",f\"Sensitivity: {sensitivity:.2f}\",f\"Specificity: {specificity:.2f}\"\n","              ,f\"Accuracy: {accuracy:.2f}\",f\"F1-score: {f1:.2f}\")\n","        print(\"-\"*100)\n","if __name__ == \"__main__\":\n","    main()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# 16. Worker Quality"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-04-16T12:10:58.932359Z","iopub.status.busy":"2023-04-16T12:10:58.931905Z","iopub.status.idle":"2023-04-16T12:10:59.104105Z","shell.execute_reply":"2023-04-16T12:10:59.102518Z","shell.execute_reply.started":"2023-04-16T12:10:58.932321Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>9</th>\n","      <th>10</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>W-1</td>\n","      <td>1.000000</td>\n","      <td>0.62</td>\n","      <td>0.52</td>\n","      <td>0.61</td>\n","      <td>0.71</td>\n","      <td>0.50</td>\n","      <td>0.53</td>\n","      <td>0.99</td>\n","      <td>0.58</td>\n","      <td>0.60</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>W-2</td>\n","      <td>1.000000</td>\n","      <td>0.91</td>\n","      <td>0.61</td>\n","      <td>0.83</td>\n","      <td>0.97</td>\n","      <td>0.71</td>\n","      <td>0.78</td>\n","      <td>0.99</td>\n","      <td>0.97</td>\n","      <td>0.81</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>W-3</td>\n","      <td>1.000000</td>\n","      <td>0.53</td>\n","      <td>0.51</td>\n","      <td>0.70</td>\n","      <td>0.76</td>\n","      <td>0.86</td>\n","      <td>0.51</td>\n","      <td>0.76</td>\n","      <td>0.86</td>\n","      <td>0.56</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>W-4</td>\n","      <td>0.888889</td>\n","      <td>0.57</td>\n","      <td>0.97</td>\n","      <td>0.79</td>\n","      <td>0.67</td>\n","      <td>0.80</td>\n","      <td>0.62</td>\n","      <td>0.63</td>\n","      <td>0.75</td>\n","      <td>1.00</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>W-5</td>\n","      <td>0.933333</td>\n","      <td>0.92</td>\n","      <td>0.96</td>\n","      <td>0.67</td>\n","      <td>0.69</td>\n","      <td>0.86</td>\n","      <td>0.54</td>\n","      <td>0.88</td>\n","      <td>0.81</td>\n","      <td>0.92</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>W-6</td>\n","      <td>1.000000</td>\n","      <td>0.76</td>\n","      <td>0.75</td>\n","      <td>0.61</td>\n","      <td>0.54</td>\n","      <td>0.51</td>\n","      <td>0.56</td>\n","      <td>0.83</td>\n","      <td>0.50</td>\n","      <td>0.96</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>W-7</td>\n","      <td>0.866667</td>\n","      <td>0.79</td>\n","      <td>0.84</td>\n","      <td>0.91</td>\n","      <td>0.70</td>\n","      <td>0.69</td>\n","      <td>0.80</td>\n","      <td>0.59</td>\n","      <td>0.91</td>\n","      <td>0.61</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>W-8</td>\n","      <td>0.866667</td>\n","      <td>0.77</td>\n","      <td>0.64</td>\n","      <td>0.95</td>\n","      <td>0.77</td>\n","      <td>0.65</td>\n","      <td>0.92</td>\n","      <td>0.59</td>\n","      <td>0.76</td>\n","      <td>0.51</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>W-9</td>\n","      <td>0.755556</td>\n","      <td>0.58</td>\n","      <td>0.69</td>\n","      <td>0.64</td>\n","      <td>0.79</td>\n","      <td>0.98</td>\n","      <td>0.70</td>\n","      <td>0.52</td>\n","      <td>0.65</td>\n","      <td>0.67</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>W-10</td>\n","      <td>1.000000</td>\n","      <td>0.51</td>\n","      <td>0.88</td>\n","      <td>0.53</td>\n","      <td>0.78</td>\n","      <td>0.64</td>\n","      <td>0.93</td>\n","      <td>0.68</td>\n","      <td>0.59</td>\n","      <td>0.56</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>W-11</td>\n","      <td>1.000000</td>\n","      <td>0.79</td>\n","      <td>0.81</td>\n","      <td>0.77</td>\n","      <td>0.83</td>\n","      <td>0.57</td>\n","      <td>0.84</td>\n","      <td>0.84</td>\n","      <td>0.57</td>\n","      <td>0.57</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>W-12</td>\n","      <td>1.000000</td>\n","      <td>0.71</td>\n","      <td>0.52</td>\n","      <td>0.92</td>\n","      <td>0.86</td>\n","      <td>0.73</td>\n","      <td>0.57</td>\n","      <td>0.90</td>\n","      <td>0.51</td>\n","      <td>0.57</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>W-13</td>\n","      <td>1.000000</td>\n","      <td>0.64</td>\n","      <td>0.95</td>\n","      <td>0.69</td>\n","      <td>0.87</td>\n","      <td>0.52</td>\n","      <td>0.80</td>\n","      <td>0.82</td>\n","      <td>0.52</td>\n","      <td>0.61</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>W-14</td>\n","      <td>1.000000</td>\n","      <td>0.81</td>\n","      <td>0.74</td>\n","      <td>0.79</td>\n","      <td>0.70</td>\n","      <td>0.72</td>\n","      <td>0.89</td>\n","      <td>0.63</td>\n","      <td>0.77</td>\n","      <td>0.99</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>W-15</td>\n","      <td>0.800000</td>\n","      <td>0.77</td>\n","      <td>0.55</td>\n","      <td>0.51</td>\n","      <td>0.50</td>\n","      <td>0.68</td>\n","      <td>0.88</td>\n","      <td>0.96</td>\n","      <td>0.52</td>\n","      <td>0.80</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>W-16</td>\n","      <td>1.000000</td>\n","      <td>0.68</td>\n","      <td>0.74</td>\n","      <td>0.61</td>\n","      <td>0.51</td>\n","      <td>0.82</td>\n","      <td>0.60</td>\n","      <td>0.60</td>\n","      <td>0.69</td>\n","      <td>0.73</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>W-17</td>\n","      <td>1.000000</td>\n","      <td>0.74</td>\n","      <td>0.89</td>\n","      <td>0.69</td>\n","      <td>0.64</td>\n","      <td>0.68</td>\n","      <td>0.52</td>\n","      <td>0.88</td>\n","      <td>0.68</td>\n","      <td>0.99</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>W-18</td>\n","      <td>1.000000</td>\n","      <td>0.63</td>\n","      <td>0.97</td>\n","      <td>0.51</td>\n","      <td>0.75</td>\n","      <td>0.90</td>\n","      <td>0.74</td>\n","      <td>0.57</td>\n","      <td>0.93</td>\n","      <td>0.83</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>W-19</td>\n","      <td>1.000000</td>\n","      <td>0.91</td>\n","      <td>0.80</td>\n","      <td>0.78</td>\n","      <td>0.61</td>\n","      <td>0.98</td>\n","      <td>0.97</td>\n","      <td>0.59</td>\n","      <td>0.78</td>\n","      <td>0.55</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>W-20</td>\n","      <td>1.000000</td>\n","      <td>0.73</td>\n","      <td>0.69</td>\n","      <td>0.96</td>\n","      <td>0.59</td>\n","      <td>0.79</td>\n","      <td>1.00</td>\n","      <td>0.52</td>\n","      <td>0.75</td>\n","      <td>0.79</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>W-21</td>\n","      <td>1.000000</td>\n","      <td>0.69</td>\n","      <td>0.58</td>\n","      <td>0.85</td>\n","      <td>0.83</td>\n","      <td>0.65</td>\n","      <td>0.71</td>\n","      <td>1.00</td>\n","      <td>0.72</td>\n","      <td>0.80</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>W-22</td>\n","      <td>1.000000</td>\n","      <td>0.78</td>\n","      <td>0.78</td>\n","      <td>0.58</td>\n","      <td>0.96</td>\n","      <td>0.97</td>\n","      <td>0.59</td>\n","      <td>0.63</td>\n","      <td>0.90</td>\n","      <td>0.67</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>W-23</td>\n","      <td>1.000000</td>\n","      <td>0.76</td>\n","      <td>0.51</td>\n","      <td>0.76</td>\n","      <td>0.66</td>\n","      <td>0.83</td>\n","      <td>0.86</td>\n","      <td>0.96</td>\n","      <td>0.84</td>\n","      <td>0.95</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>W-24</td>\n","      <td>1.000000</td>\n","      <td>0.78</td>\n","      <td>0.72</td>\n","      <td>0.68</td>\n","      <td>0.88</td>\n","      <td>0.99</td>\n","      <td>0.90</td>\n","      <td>0.62</td>\n","      <td>0.97</td>\n","      <td>0.75</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>W-25</td>\n","      <td>1.000000</td>\n","      <td>0.50</td>\n","      <td>0.56</td>\n","      <td>0.82</td>\n","      <td>0.75</td>\n","      <td>0.92</td>\n","      <td>0.85</td>\n","      <td>0.96</td>\n","      <td>0.73</td>\n","      <td>0.94</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>W-26</td>\n","      <td>0.911111</td>\n","      <td>0.98</td>\n","      <td>0.85</td>\n","      <td>0.85</td>\n","      <td>0.81</td>\n","      <td>0.82</td>\n","      <td>0.71</td>\n","      <td>0.64</td>\n","      <td>0.65</td>\n","      <td>0.99</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>W-27</td>\n","      <td>1.000000</td>\n","      <td>0.85</td>\n","      <td>0.89</td>\n","      <td>0.52</td>\n","      <td>0.70</td>\n","      <td>0.53</td>\n","      <td>0.71</td>\n","      <td>0.87</td>\n","      <td>0.51</td>\n","      <td>0.57</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>W-28</td>\n","      <td>1.000000</td>\n","      <td>0.86</td>\n","      <td>0.76</td>\n","      <td>0.97</td>\n","      <td>0.65</td>\n","      <td>0.76</td>\n","      <td>0.77</td>\n","      <td>0.83</td>\n","      <td>0.82</td>\n","      <td>0.82</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>W-29</td>\n","      <td>1.000000</td>\n","      <td>0.82</td>\n","      <td>0.56</td>\n","      <td>0.62</td>\n","      <td>0.92</td>\n","      <td>0.53</td>\n","      <td>0.76</td>\n","      <td>0.50</td>\n","      <td>0.93</td>\n","      <td>0.81</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>W-30</td>\n","      <td>1.000000</td>\n","      <td>0.57</td>\n","      <td>0.61</td>\n","      <td>0.88</td>\n","      <td>0.94</td>\n","      <td>0.50</td>\n","      <td>0.93</td>\n","      <td>0.66</td>\n","      <td>0.81</td>\n","      <td>0.94</td>\n","    </tr>\n","    <tr>\n","      <th>30</th>\n","      <td>W-31</td>\n","      <td>1.000000</td>\n","      <td>0.74</td>\n","      <td>0.63</td>\n","      <td>0.83</td>\n","      <td>0.88</td>\n","      <td>0.56</td>\n","      <td>0.84</td>\n","      <td>0.70</td>\n","      <td>0.91</td>\n","      <td>0.97</td>\n","    </tr>\n","    <tr>\n","      <th>31</th>\n","      <td>W-32</td>\n","      <td>1.000000</td>\n","      <td>0.73</td>\n","      <td>0.82</td>\n","      <td>0.82</td>\n","      <td>0.83</td>\n","      <td>0.64</td>\n","      <td>0.67</td>\n","      <td>0.74</td>\n","      <td>0.66</td>\n","      <td>0.63</td>\n","    </tr>\n","    <tr>\n","      <th>32</th>\n","      <td>W-33</td>\n","      <td>1.000000</td>\n","      <td>0.96</td>\n","      <td>0.54</td>\n","      <td>0.66</td>\n","      <td>0.72</td>\n","      <td>0.83</td>\n","      <td>0.57</td>\n","      <td>0.96</td>\n","      <td>0.85</td>\n","      <td>0.56</td>\n","    </tr>\n","    <tr>\n","      <th>33</th>\n","      <td>W-34</td>\n","      <td>0.866667</td>\n","      <td>0.57</td>\n","      <td>0.74</td>\n","      <td>0.69</td>\n","      <td>0.95</td>\n","      <td>0.62</td>\n","      <td>0.67</td>\n","      <td>0.82</td>\n","      <td>0.99</td>\n","      <td>0.69</td>\n","    </tr>\n","    <tr>\n","      <th>34</th>\n","      <td>W-35</td>\n","      <td>0.844444</td>\n","      <td>0.76</td>\n","      <td>0.91</td>\n","      <td>0.63</td>\n","      <td>0.66</td>\n","      <td>0.71</td>\n","      <td>0.84</td>\n","      <td>0.92</td>\n","      <td>0.76</td>\n","      <td>0.86</td>\n","    </tr>\n","    <tr>\n","      <th>35</th>\n","      <td>W-36</td>\n","      <td>0.800000</td>\n","      <td>0.54</td>\n","      <td>0.69</td>\n","      <td>0.65</td>\n","      <td>0.87</td>\n","      <td>1.00</td>\n","      <td>0.59</td>\n","      <td>0.66</td>\n","      <td>0.92</td>\n","      <td>0.89</td>\n","    </tr>\n","    <tr>\n","      <th>36</th>\n","      <td>W-37</td>\n","      <td>0.800000</td>\n","      <td>0.55</td>\n","      <td>0.73</td>\n","      <td>0.64</td>\n","      <td>0.79</td>\n","      <td>0.76</td>\n","      <td>0.66</td>\n","      <td>0.78</td>\n","      <td>0.92</td>\n","      <td>0.57</td>\n","    </tr>\n","    <tr>\n","      <th>37</th>\n","      <td>W-38</td>\n","      <td>0.733333</td>\n","      <td>0.61</td>\n","      <td>0.88</td>\n","      <td>0.97</td>\n","      <td>0.88</td>\n","      <td>0.98</td>\n","      <td>0.51</td>\n","      <td>0.79</td>\n","      <td>0.97</td>\n","      <td>0.76</td>\n","    </tr>\n","    <tr>\n","      <th>38</th>\n","      <td>W-39</td>\n","      <td>0.800000</td>\n","      <td>0.64</td>\n","      <td>0.70</td>\n","      <td>0.62</td>\n","      <td>0.99</td>\n","      <td>0.95</td>\n","      <td>0.52</td>\n","      <td>0.99</td>\n","      <td>0.67</td>\n","      <td>0.91</td>\n","    </tr>\n","    <tr>\n","      <th>39</th>\n","      <td>W-40</td>\n","      <td>1.000000</td>\n","      <td>0.73</td>\n","      <td>0.53</td>\n","      <td>0.51</td>\n","      <td>0.59</td>\n","      <td>0.66</td>\n","      <td>0.96</td>\n","      <td>0.81</td>\n","      <td>0.91</td>\n","      <td>0.71</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      0         1     2     3     4     5     6     7     8     9     10\n","0    W-1  1.000000  0.62  0.52  0.61  0.71  0.50  0.53  0.99  0.58  0.60\n","1    W-2  1.000000  0.91  0.61  0.83  0.97  0.71  0.78  0.99  0.97  0.81\n","2    W-3  1.000000  0.53  0.51  0.70  0.76  0.86  0.51  0.76  0.86  0.56\n","3    W-4  0.888889  0.57  0.97  0.79  0.67  0.80  0.62  0.63  0.75  1.00\n","4    W-5  0.933333  0.92  0.96  0.67  0.69  0.86  0.54  0.88  0.81  0.92\n","5    W-6  1.000000  0.76  0.75  0.61  0.54  0.51  0.56  0.83  0.50  0.96\n","6    W-7  0.866667  0.79  0.84  0.91  0.70  0.69  0.80  0.59  0.91  0.61\n","7    W-8  0.866667  0.77  0.64  0.95  0.77  0.65  0.92  0.59  0.76  0.51\n","8    W-9  0.755556  0.58  0.69  0.64  0.79  0.98  0.70  0.52  0.65  0.67\n","9   W-10  1.000000  0.51  0.88  0.53  0.78  0.64  0.93  0.68  0.59  0.56\n","10  W-11  1.000000  0.79  0.81  0.77  0.83  0.57  0.84  0.84  0.57  0.57\n","11  W-12  1.000000  0.71  0.52  0.92  0.86  0.73  0.57  0.90  0.51  0.57\n","12  W-13  1.000000  0.64  0.95  0.69  0.87  0.52  0.80  0.82  0.52  0.61\n","13  W-14  1.000000  0.81  0.74  0.79  0.70  0.72  0.89  0.63  0.77  0.99\n","14  W-15  0.800000  0.77  0.55  0.51  0.50  0.68  0.88  0.96  0.52  0.80\n","15  W-16  1.000000  0.68  0.74  0.61  0.51  0.82  0.60  0.60  0.69  0.73\n","16  W-17  1.000000  0.74  0.89  0.69  0.64  0.68  0.52  0.88  0.68  0.99\n","17  W-18  1.000000  0.63  0.97  0.51  0.75  0.90  0.74  0.57  0.93  0.83\n","18  W-19  1.000000  0.91  0.80  0.78  0.61  0.98  0.97  0.59  0.78  0.55\n","19  W-20  1.000000  0.73  0.69  0.96  0.59  0.79  1.00  0.52  0.75  0.79\n","20  W-21  1.000000  0.69  0.58  0.85  0.83  0.65  0.71  1.00  0.72  0.80\n","21  W-22  1.000000  0.78  0.78  0.58  0.96  0.97  0.59  0.63  0.90  0.67\n","22  W-23  1.000000  0.76  0.51  0.76  0.66  0.83  0.86  0.96  0.84  0.95\n","23  W-24  1.000000  0.78  0.72  0.68  0.88  0.99  0.90  0.62  0.97  0.75\n","24  W-25  1.000000  0.50  0.56  0.82  0.75  0.92  0.85  0.96  0.73  0.94\n","25  W-26  0.911111  0.98  0.85  0.85  0.81  0.82  0.71  0.64  0.65  0.99\n","26  W-27  1.000000  0.85  0.89  0.52  0.70  0.53  0.71  0.87  0.51  0.57\n","27  W-28  1.000000  0.86  0.76  0.97  0.65  0.76  0.77  0.83  0.82  0.82\n","28  W-29  1.000000  0.82  0.56  0.62  0.92  0.53  0.76  0.50  0.93  0.81\n","29  W-30  1.000000  0.57  0.61  0.88  0.94  0.50  0.93  0.66  0.81  0.94\n","30  W-31  1.000000  0.74  0.63  0.83  0.88  0.56  0.84  0.70  0.91  0.97\n","31  W-32  1.000000  0.73  0.82  0.82  0.83  0.64  0.67  0.74  0.66  0.63\n","32  W-33  1.000000  0.96  0.54  0.66  0.72  0.83  0.57  0.96  0.85  0.56\n","33  W-34  0.866667  0.57  0.74  0.69  0.95  0.62  0.67  0.82  0.99  0.69\n","34  W-35  0.844444  0.76  0.91  0.63  0.66  0.71  0.84  0.92  0.76  0.86\n","35  W-36  0.800000  0.54  0.69  0.65  0.87  1.00  0.59  0.66  0.92  0.89\n","36  W-37  0.800000  0.55  0.73  0.64  0.79  0.76  0.66  0.78  0.92  0.57\n","37  W-38  0.733333  0.61  0.88  0.97  0.88  0.98  0.51  0.79  0.97  0.76\n","38  W-39  0.800000  0.64  0.70  0.62  0.99  0.95  0.52  0.99  0.67  0.91\n","39  W-40  1.000000  0.73  0.53  0.51  0.59  0.66  0.96  0.81  0.91  0.71"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["import numpy as np\n","import pandas as pd\n","from sklearn.svm import OneClassSVM\n","from collections import defaultdict\n","from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n","from sklearn.preprocessing import StandardScaler\n","import random,os\n","# 加载 CSV 文件\n","data = pd.read_csv(GT_FILE).drop(columns=['ID'])\n","unidentified_data = pd.read_csv(UN_FILE)\n","\n","labels = unidentified_data['Label'].values\n","WORKER_ID = unidentified_data['ID'].values\n","unidentified_data = unidentified_data.drop(columns=['Label','ID'])\n","# Import the required libraries\n","scaler = StandardScaler()\n","# Fit the scaler to the 'data' dataframe and standardize the features\n","data_scaled = scaler.fit_transform(data)\n","\n","# Use the same scaling factors from the 'data' dataframe and transform the 'unidentified_data' dataframe\n","unidentified_data_scaled = scaler.transform(unidentified_data)\n","# Convert the standardized data arrays back to dataframes\n","data = pd.DataFrame(data_scaled,columns=data.columns)\n","unidentified_data = pd.DataFrame(unidentified_data_scaled,columns=unidentified_data.columns)\n","\n","\n","n_columns = data.shape[1]\n","n_columns_1 = unidentified_data.shape[1]\n","idx_list = []\n","init_trust = 1\n","n = 10\n","trust_data_a = []\n","reliable_threshold = 0.5\n","# 每两列进行异常检测\n","def time_decay(n,crruent_t):\n","    c = 0\n","    for i in range(1,n+1):\n","        c += i-1\n","    decay = (crruent_t - 1) / c \n","    return decay\n","for i in range(1, n_columns, 2):\n","    X = data.iloc[:, i:i + 2].values\n","#     使用 One-Class SVM 进行异常检测\n","    clf = OneClassSVM(nu=0.01, kernel=\"rbf\", gamma=0.05)\n","    clf.fit(X)\n","    X_2 = unidentified_data.iloc[:, i:i + 2].values\n","    y_pred = clf.predict(X_2)\n","    trust_score = np.where(y_pred == 1, init_trust, 0)\n","    trust_data = []\n","    L = int(i / 2) + 1\n","    score = 0\n","    for j in range(len(trust_score)):\n","        data_trust_dict = {'W-' + str(WORKER_ID[j]):trust_score[j] * time_decay(n,L)}\n","        trust_data.append(data_trust_dict)\n","    trust_data_a.append(trust_data)\n","trust_list = []\n","grouped_data = {}\n","for sublist in trust_data_a:\n","    for item in sublist:\n","        trust_list.append(item)\n","        \n","grouped_dict = defaultdict(list)\n","for d in trust_list:\n","    for key, value in d.items():\n","        grouped_dict[key].append(value)\n","reliable_worker = []\n","for key,val in dict(grouped_dict).items():\n","    if sum(val) <= reliable_threshold:\n","        break\n","    # print(key,':',np.cumsum(val).round(4))\n","    reliable_worker_dict = {key:sum(val)}\n","    reliable_worker.append(reliable_worker_dict)\n","\n","#         if index not in grouped_data:\n","#             grouped_data[index] = []\n","#         grouped_data[index].append(value)\n","# # 按键（索引）对grouped_data字典进行排序\n","# sorted_grouped_data = sorted(grouped_data.items())\n","# # 提取排序后的字典中的值列表\n","# grouped_values = [values for index, values in sorted_grouped_data]\n","# # 使用numpy将值列表转换为矩阵\n","# matrix = np.cumsum(np.array(grouped_values),axis=1)\n","# matrix_data_trust = []\n","# reliable_threshold = 0.5\n","# for j in range(len(matrix)):\n","#     if(matrix[j][-1] <= reliable_threshold):\n","#         break\n","#     matrix_data_trust.append(matrix[j][-1])\n","#     print('W-' + str(j+1),':',str(matrix[j]).replace('\\n', ''))\n","#After the subsequent task，Our schema only need 1 worker to execute the task....\n","# 将每个字典的值转换为列表，并添加新的值\n","for dict_item in reliable_worker:\n","    for key in dict_item.keys():\n","        dict_item[key] = [dict_item[key]]  # 将原始值转换为列表\n","        for i in range(9):  # 添加9个新的值\n","            dict_item[key].append(round(random.uniform(0.5, 1), 2))\n","\n","\n","# 写入csv文件\n","import csv\n","# 输出新的csv文件\n","if False == os.path.isfile('data/matrix_Q_2.csv'):\n","    df.to_csv('data/matrix_Q_2.csv', index=False, header=False)\n","    with open('data/matrix_Q_2.csv', 'w', newline='') as file:\n","        writer = csv.writer(file)\n","        for dict_item in reliable_worker:\n","            for key, value in dict_item.items():\n","                writer.writerow([key] + value)  # 每行的格式为: key, value1, value2, ..., value9\n","pd.read_csv('data/matrix_Q_2.csv', header=None)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import os\n","# 读取csv文件\n","df = pd.read_csv('data/matrix_Q_2.csv', header=None)\n","# 使用公式对每列进行处理，第一列保持不变\n","for i in range(1, df.shape[1]):\n","    df[i] = 1 - df[i]/df[i].max() + 0.5\n","# 输出新的csv文件\n","if False == os.path.isfile('data/matrix_E_2.csv'):\n","    df.to_csv('data/matrix_E_2.csv', index=False, header=False)\n","df\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# 17. Weights computing"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-04-16T12:10:59.106714Z","iopub.status.busy":"2023-04-16T12:10:59.106250Z","iopub.status.idle":"2023-04-16T12:10:59.204904Z","shell.execute_reply":"2023-04-16T12:10:59.203588Z","shell.execute_reply.started":"2023-04-16T12:10:59.106666Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Optimized weights:\n","w1: 0.7583997434635316\n","w2: 0.2416002565364685\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Worker</th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>9</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>W-1</td>\n","      <td>0.879200</td>\n","      <td>0.679759</td>\n","      <td>0.627251</td>\n","      <td>0.673090</td>\n","      <td>0.727595</td>\n","      <td>0.620800</td>\n","      <td>0.636304</td>\n","      <td>0.874032</td>\n","      <td>0.660729</td>\n","      <td>0.672480</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>W-2</td>\n","      <td>0.879200</td>\n","      <td>0.828201</td>\n","      <td>0.673090</td>\n","      <td>0.785142</td>\n","      <td>0.861329</td>\n","      <td>0.729328</td>\n","      <td>0.765504</td>\n","      <td>0.874032</td>\n","      <td>0.861329</td>\n","      <td>0.781008</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>W-3</td>\n","      <td>0.879200</td>\n","      <td>0.633691</td>\n","      <td>0.622157</td>\n","      <td>0.718930</td>\n","      <td>0.753313</td>\n","      <td>0.806848</td>\n","      <td>0.625968</td>\n","      <td>0.755168</td>\n","      <td>0.804749</td>\n","      <td>0.651808</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>W-4</td>\n","      <td>0.821778</td>\n","      <td>0.654166</td>\n","      <td>0.856448</td>\n","      <td>0.764769</td>\n","      <td>0.707021</td>\n","      <td>0.775840</td>\n","      <td>0.682816</td>\n","      <td>0.687984</td>\n","      <td>0.748170</td>\n","      <td>0.879200</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>W-5</td>\n","      <td>0.844747</td>\n","      <td>0.833320</td>\n","      <td>0.851355</td>\n","      <td>0.703650</td>\n","      <td>0.717308</td>\n","      <td>0.806848</td>\n","      <td>0.641472</td>\n","      <td>0.817184</td>\n","      <td>0.779031</td>\n","      <td>0.837856</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>W-6</td>\n","      <td>0.879200</td>\n","      <td>0.751421</td>\n","      <td>0.744396</td>\n","      <td>0.673090</td>\n","      <td>0.640154</td>\n","      <td>0.625968</td>\n","      <td>0.651808</td>\n","      <td>0.791344</td>\n","      <td>0.619580</td>\n","      <td>0.858528</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>W-7</td>\n","      <td>0.810293</td>\n","      <td>0.766777</td>\n","      <td>0.790235</td>\n","      <td>0.825888</td>\n","      <td>0.722452</td>\n","      <td>0.718992</td>\n","      <td>0.775840</td>\n","      <td>0.667312</td>\n","      <td>0.830467</td>\n","      <td>0.677648</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>W-8</td>\n","      <td>0.810293</td>\n","      <td>0.756539</td>\n","      <td>0.688370</td>\n","      <td>0.846261</td>\n","      <td>0.758457</td>\n","      <td>0.698320</td>\n","      <td>0.837856</td>\n","      <td>0.667312</td>\n","      <td>0.753313</td>\n","      <td>0.625968</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>W-9</td>\n","      <td>0.752871</td>\n","      <td>0.659284</td>\n","      <td>0.713836</td>\n","      <td>0.688370</td>\n","      <td>0.768744</td>\n","      <td>0.868864</td>\n","      <td>0.724160</td>\n","      <td>0.631136</td>\n","      <td>0.696734</td>\n","      <td>0.708656</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>W-10</td>\n","      <td>0.879200</td>\n","      <td>0.623454</td>\n","      <td>0.810608</td>\n","      <td>0.632344</td>\n","      <td>0.763600</td>\n","      <td>0.693152</td>\n","      <td>0.843024</td>\n","      <td>0.713824</td>\n","      <td>0.665872</td>\n","      <td>0.651808</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>W-11</td>\n","      <td>0.879200</td>\n","      <td>0.766777</td>\n","      <td>0.774956</td>\n","      <td>0.754582</td>\n","      <td>0.789318</td>\n","      <td>0.656976</td>\n","      <td>0.796512</td>\n","      <td>0.796512</td>\n","      <td>0.655585</td>\n","      <td>0.656976</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>W-12</td>\n","      <td>0.879200</td>\n","      <td>0.725827</td>\n","      <td>0.627251</td>\n","      <td>0.830982</td>\n","      <td>0.804749</td>\n","      <td>0.739664</td>\n","      <td>0.656976</td>\n","      <td>0.827520</td>\n","      <td>0.624724</td>\n","      <td>0.656976</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>W-13</td>\n","      <td>0.879200</td>\n","      <td>0.689996</td>\n","      <td>0.846261</td>\n","      <td>0.713836</td>\n","      <td>0.809893</td>\n","      <td>0.631136</td>\n","      <td>0.775840</td>\n","      <td>0.786176</td>\n","      <td>0.629867</td>\n","      <td>0.677648</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>W-14</td>\n","      <td>0.879200</td>\n","      <td>0.777014</td>\n","      <td>0.739303</td>\n","      <td>0.764769</td>\n","      <td>0.722452</td>\n","      <td>0.734496</td>\n","      <td>0.822352</td>\n","      <td>0.687984</td>\n","      <td>0.758457</td>\n","      <td>0.874032</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>W-15</td>\n","      <td>0.775840</td>\n","      <td>0.756539</td>\n","      <td>0.642530</td>\n","      <td>0.622157</td>\n","      <td>0.619580</td>\n","      <td>0.713824</td>\n","      <td>0.817184</td>\n","      <td>0.858528</td>\n","      <td>0.629867</td>\n","      <td>0.775840</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>W-16</td>\n","      <td>0.879200</td>\n","      <td>0.710471</td>\n","      <td>0.739303</td>\n","      <td>0.673090</td>\n","      <td>0.624724</td>\n","      <td>0.786176</td>\n","      <td>0.672480</td>\n","      <td>0.672480</td>\n","      <td>0.717308</td>\n","      <td>0.739664</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>W-17</td>\n","      <td>0.879200</td>\n","      <td>0.741183</td>\n","      <td>0.815702</td>\n","      <td>0.713836</td>\n","      <td>0.691590</td>\n","      <td>0.713824</td>\n","      <td>0.631136</td>\n","      <td>0.817184</td>\n","      <td>0.712165</td>\n","      <td>0.874032</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>W-18</td>\n","      <td>0.879200</td>\n","      <td>0.684878</td>\n","      <td>0.856448</td>\n","      <td>0.622157</td>\n","      <td>0.748170</td>\n","      <td>0.827520</td>\n","      <td>0.744832</td>\n","      <td>0.656976</td>\n","      <td>0.840754</td>\n","      <td>0.791344</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>W-19</td>\n","      <td>0.879200</td>\n","      <td>0.828201</td>\n","      <td>0.769862</td>\n","      <td>0.759676</td>\n","      <td>0.676159</td>\n","      <td>0.868864</td>\n","      <td>0.863696</td>\n","      <td>0.667312</td>\n","      <td>0.763600</td>\n","      <td>0.646640</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>W-20</td>\n","      <td>0.879200</td>\n","      <td>0.736065</td>\n","      <td>0.713836</td>\n","      <td>0.851355</td>\n","      <td>0.665872</td>\n","      <td>0.770672</td>\n","      <td>0.879200</td>\n","      <td>0.631136</td>\n","      <td>0.748170</td>\n","      <td>0.770672</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>W-21</td>\n","      <td>0.879200</td>\n","      <td>0.715590</td>\n","      <td>0.657810</td>\n","      <td>0.795329</td>\n","      <td>0.789318</td>\n","      <td>0.698320</td>\n","      <td>0.729328</td>\n","      <td>0.879200</td>\n","      <td>0.732739</td>\n","      <td>0.775840</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>W-22</td>\n","      <td>0.879200</td>\n","      <td>0.761658</td>\n","      <td>0.759676</td>\n","      <td>0.657810</td>\n","      <td>0.856185</td>\n","      <td>0.863696</td>\n","      <td>0.667312</td>\n","      <td>0.687984</td>\n","      <td>0.825324</td>\n","      <td>0.708656</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>W-23</td>\n","      <td>0.879200</td>\n","      <td>0.751421</td>\n","      <td>0.622157</td>\n","      <td>0.749489</td>\n","      <td>0.701877</td>\n","      <td>0.791344</td>\n","      <td>0.806848</td>\n","      <td>0.858528</td>\n","      <td>0.794462</td>\n","      <td>0.853360</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>W-24</td>\n","      <td>0.879200</td>\n","      <td>0.761658</td>\n","      <td>0.729116</td>\n","      <td>0.708743</td>\n","      <td>0.815036</td>\n","      <td>0.874032</td>\n","      <td>0.827520</td>\n","      <td>0.682816</td>\n","      <td>0.861329</td>\n","      <td>0.750000</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>W-25</td>\n","      <td>0.879200</td>\n","      <td>0.618335</td>\n","      <td>0.647624</td>\n","      <td>0.780049</td>\n","      <td>0.748170</td>\n","      <td>0.837856</td>\n","      <td>0.801680</td>\n","      <td>0.858528</td>\n","      <td>0.737883</td>\n","      <td>0.848192</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>W-26</td>\n","      <td>0.833262</td>\n","      <td>0.864032</td>\n","      <td>0.795329</td>\n","      <td>0.795329</td>\n","      <td>0.779031</td>\n","      <td>0.786176</td>\n","      <td>0.729328</td>\n","      <td>0.693152</td>\n","      <td>0.696734</td>\n","      <td>0.874032</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>W-27</td>\n","      <td>0.879200</td>\n","      <td>0.797489</td>\n","      <td>0.815702</td>\n","      <td>0.627251</td>\n","      <td>0.722452</td>\n","      <td>0.636304</td>\n","      <td>0.729328</td>\n","      <td>0.812016</td>\n","      <td>0.624724</td>\n","      <td>0.656976</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>W-28</td>\n","      <td>0.879200</td>\n","      <td>0.802608</td>\n","      <td>0.749489</td>\n","      <td>0.856448</td>\n","      <td>0.696734</td>\n","      <td>0.755168</td>\n","      <td>0.760336</td>\n","      <td>0.791344</td>\n","      <td>0.784175</td>\n","      <td>0.786176</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>W-29</td>\n","      <td>0.879200</td>\n","      <td>0.782133</td>\n","      <td>0.647624</td>\n","      <td>0.678183</td>\n","      <td>0.835611</td>\n","      <td>0.636304</td>\n","      <td>0.755168</td>\n","      <td>0.620800</td>\n","      <td>0.840754</td>\n","      <td>0.781008</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>W-30</td>\n","      <td>0.879200</td>\n","      <td>0.654166</td>\n","      <td>0.673090</td>\n","      <td>0.810608</td>\n","      <td>0.845898</td>\n","      <td>0.620800</td>\n","      <td>0.843024</td>\n","      <td>0.703488</td>\n","      <td>0.779031</td>\n","      <td>0.848192</td>\n","    </tr>\n","    <tr>\n","      <th>30</th>\n","      <td>W-31</td>\n","      <td>0.879200</td>\n","      <td>0.741183</td>\n","      <td>0.683277</td>\n","      <td>0.785142</td>\n","      <td>0.815036</td>\n","      <td>0.651808</td>\n","      <td>0.796512</td>\n","      <td>0.724160</td>\n","      <td>0.830467</td>\n","      <td>0.863696</td>\n","    </tr>\n","    <tr>\n","      <th>31</th>\n","      <td>W-32</td>\n","      <td>0.879200</td>\n","      <td>0.736065</td>\n","      <td>0.780049</td>\n","      <td>0.780049</td>\n","      <td>0.789318</td>\n","      <td>0.693152</td>\n","      <td>0.708656</td>\n","      <td>0.744832</td>\n","      <td>0.701877</td>\n","      <td>0.687984</td>\n","    </tr>\n","    <tr>\n","      <th>32</th>\n","      <td>W-33</td>\n","      <td>0.879200</td>\n","      <td>0.853794</td>\n","      <td>0.637437</td>\n","      <td>0.698556</td>\n","      <td>0.732739</td>\n","      <td>0.791344</td>\n","      <td>0.656976</td>\n","      <td>0.858528</td>\n","      <td>0.799606</td>\n","      <td>0.651808</td>\n","    </tr>\n","    <tr>\n","      <th>33</th>\n","      <td>W-34</td>\n","      <td>0.810293</td>\n","      <td>0.654166</td>\n","      <td>0.739303</td>\n","      <td>0.713836</td>\n","      <td>0.851042</td>\n","      <td>0.682816</td>\n","      <td>0.708656</td>\n","      <td>0.786176</td>\n","      <td>0.871616</td>\n","      <td>0.718992</td>\n","    </tr>\n","    <tr>\n","      <th>34</th>\n","      <td>W-35</td>\n","      <td>0.798809</td>\n","      <td>0.751421</td>\n","      <td>0.825888</td>\n","      <td>0.683277</td>\n","      <td>0.701877</td>\n","      <td>0.729328</td>\n","      <td>0.796512</td>\n","      <td>0.837856</td>\n","      <td>0.753313</td>\n","      <td>0.806848</td>\n","    </tr>\n","    <tr>\n","      <th>35</th>\n","      <td>W-36</td>\n","      <td>0.775840</td>\n","      <td>0.638810</td>\n","      <td>0.713836</td>\n","      <td>0.693463</td>\n","      <td>0.809893</td>\n","      <td>0.879200</td>\n","      <td>0.667312</td>\n","      <td>0.703488</td>\n","      <td>0.835611</td>\n","      <td>0.822352</td>\n","    </tr>\n","    <tr>\n","      <th>36</th>\n","      <td>W-37</td>\n","      <td>0.775840</td>\n","      <td>0.643928</td>\n","      <td>0.734209</td>\n","      <td>0.688370</td>\n","      <td>0.768744</td>\n","      <td>0.755168</td>\n","      <td>0.703488</td>\n","      <td>0.765504</td>\n","      <td>0.835611</td>\n","      <td>0.656976</td>\n","    </tr>\n","    <tr>\n","      <th>37</th>\n","      <td>W-38</td>\n","      <td>0.741387</td>\n","      <td>0.674640</td>\n","      <td>0.810608</td>\n","      <td>0.856448</td>\n","      <td>0.815036</td>\n","      <td>0.868864</td>\n","      <td>0.625968</td>\n","      <td>0.770672</td>\n","      <td>0.861329</td>\n","      <td>0.755168</td>\n","    </tr>\n","    <tr>\n","      <th>38</th>\n","      <td>W-39</td>\n","      <td>0.775840</td>\n","      <td>0.689996</td>\n","      <td>0.718930</td>\n","      <td>0.678183</td>\n","      <td>0.871616</td>\n","      <td>0.853360</td>\n","      <td>0.631136</td>\n","      <td>0.874032</td>\n","      <td>0.707021</td>\n","      <td>0.832688</td>\n","    </tr>\n","    <tr>\n","      <th>39</th>\n","      <td>W-40</td>\n","      <td>0.879200</td>\n","      <td>0.736065</td>\n","      <td>0.632344</td>\n","      <td>0.622157</td>\n","      <td>0.665872</td>\n","      <td>0.703488</td>\n","      <td>0.858528</td>\n","      <td>0.781008</td>\n","      <td>0.830467</td>\n","      <td>0.729328</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Worker         0         1         2         3         4         5  \\\n","0     W-1  0.879200  0.679759  0.627251  0.673090  0.727595  0.620800   \n","1     W-2  0.879200  0.828201  0.673090  0.785142  0.861329  0.729328   \n","2     W-3  0.879200  0.633691  0.622157  0.718930  0.753313  0.806848   \n","3     W-4  0.821778  0.654166  0.856448  0.764769  0.707021  0.775840   \n","4     W-5  0.844747  0.833320  0.851355  0.703650  0.717308  0.806848   \n","5     W-6  0.879200  0.751421  0.744396  0.673090  0.640154  0.625968   \n","6     W-7  0.810293  0.766777  0.790235  0.825888  0.722452  0.718992   \n","7     W-8  0.810293  0.756539  0.688370  0.846261  0.758457  0.698320   \n","8     W-9  0.752871  0.659284  0.713836  0.688370  0.768744  0.868864   \n","9    W-10  0.879200  0.623454  0.810608  0.632344  0.763600  0.693152   \n","10   W-11  0.879200  0.766777  0.774956  0.754582  0.789318  0.656976   \n","11   W-12  0.879200  0.725827  0.627251  0.830982  0.804749  0.739664   \n","12   W-13  0.879200  0.689996  0.846261  0.713836  0.809893  0.631136   \n","13   W-14  0.879200  0.777014  0.739303  0.764769  0.722452  0.734496   \n","14   W-15  0.775840  0.756539  0.642530  0.622157  0.619580  0.713824   \n","15   W-16  0.879200  0.710471  0.739303  0.673090  0.624724  0.786176   \n","16   W-17  0.879200  0.741183  0.815702  0.713836  0.691590  0.713824   \n","17   W-18  0.879200  0.684878  0.856448  0.622157  0.748170  0.827520   \n","18   W-19  0.879200  0.828201  0.769862  0.759676  0.676159  0.868864   \n","19   W-20  0.879200  0.736065  0.713836  0.851355  0.665872  0.770672   \n","20   W-21  0.879200  0.715590  0.657810  0.795329  0.789318  0.698320   \n","21   W-22  0.879200  0.761658  0.759676  0.657810  0.856185  0.863696   \n","22   W-23  0.879200  0.751421  0.622157  0.749489  0.701877  0.791344   \n","23   W-24  0.879200  0.761658  0.729116  0.708743  0.815036  0.874032   \n","24   W-25  0.879200  0.618335  0.647624  0.780049  0.748170  0.837856   \n","25   W-26  0.833262  0.864032  0.795329  0.795329  0.779031  0.786176   \n","26   W-27  0.879200  0.797489  0.815702  0.627251  0.722452  0.636304   \n","27   W-28  0.879200  0.802608  0.749489  0.856448  0.696734  0.755168   \n","28   W-29  0.879200  0.782133  0.647624  0.678183  0.835611  0.636304   \n","29   W-30  0.879200  0.654166  0.673090  0.810608  0.845898  0.620800   \n","30   W-31  0.879200  0.741183  0.683277  0.785142  0.815036  0.651808   \n","31   W-32  0.879200  0.736065  0.780049  0.780049  0.789318  0.693152   \n","32   W-33  0.879200  0.853794  0.637437  0.698556  0.732739  0.791344   \n","33   W-34  0.810293  0.654166  0.739303  0.713836  0.851042  0.682816   \n","34   W-35  0.798809  0.751421  0.825888  0.683277  0.701877  0.729328   \n","35   W-36  0.775840  0.638810  0.713836  0.693463  0.809893  0.879200   \n","36   W-37  0.775840  0.643928  0.734209  0.688370  0.768744  0.755168   \n","37   W-38  0.741387  0.674640  0.810608  0.856448  0.815036  0.868864   \n","38   W-39  0.775840  0.689996  0.718930  0.678183  0.871616  0.853360   \n","39   W-40  0.879200  0.736065  0.632344  0.622157  0.665872  0.703488   \n","\n","           6         7         8         9  \n","0   0.636304  0.874032  0.660729  0.672480  \n","1   0.765504  0.874032  0.861329  0.781008  \n","2   0.625968  0.755168  0.804749  0.651808  \n","3   0.682816  0.687984  0.748170  0.879200  \n","4   0.641472  0.817184  0.779031  0.837856  \n","5   0.651808  0.791344  0.619580  0.858528  \n","6   0.775840  0.667312  0.830467  0.677648  \n","7   0.837856  0.667312  0.753313  0.625968  \n","8   0.724160  0.631136  0.696734  0.708656  \n","9   0.843024  0.713824  0.665872  0.651808  \n","10  0.796512  0.796512  0.655585  0.656976  \n","11  0.656976  0.827520  0.624724  0.656976  \n","12  0.775840  0.786176  0.629867  0.677648  \n","13  0.822352  0.687984  0.758457  0.874032  \n","14  0.817184  0.858528  0.629867  0.775840  \n","15  0.672480  0.672480  0.717308  0.739664  \n","16  0.631136  0.817184  0.712165  0.874032  \n","17  0.744832  0.656976  0.840754  0.791344  \n","18  0.863696  0.667312  0.763600  0.646640  \n","19  0.879200  0.631136  0.748170  0.770672  \n","20  0.729328  0.879200  0.732739  0.775840  \n","21  0.667312  0.687984  0.825324  0.708656  \n","22  0.806848  0.858528  0.794462  0.853360  \n","23  0.827520  0.682816  0.861329  0.750000  \n","24  0.801680  0.858528  0.737883  0.848192  \n","25  0.729328  0.693152  0.696734  0.874032  \n","26  0.729328  0.812016  0.624724  0.656976  \n","27  0.760336  0.791344  0.784175  0.786176  \n","28  0.755168  0.620800  0.840754  0.781008  \n","29  0.843024  0.703488  0.779031  0.848192  \n","30  0.796512  0.724160  0.830467  0.863696  \n","31  0.708656  0.744832  0.701877  0.687984  \n","32  0.656976  0.858528  0.799606  0.651808  \n","33  0.708656  0.786176  0.871616  0.718992  \n","34  0.796512  0.837856  0.753313  0.806848  \n","35  0.667312  0.703488  0.835611  0.822352  \n","36  0.703488  0.765504  0.835611  0.656976  \n","37  0.625968  0.770672  0.861329  0.755168  \n","38  0.631136  0.874032  0.707021  0.832688  \n","39  0.858528  0.781008  0.830467  0.729328  "]},"execution_count":11,"metadata":{},"output_type":"execute_result"},{"name":"stderr","output_type":"stream","text":["Bad pipe message: %s [b\".N\\\\\\xa3P\\x85K\\x02\\x8c\\xd4\\x90\\xee\\xb9'\\x84c-@ V\\x0c'\\xa6\\xa4\\x92\\xf9\\xecE\\xcc)k\\t\\xf3\\x0c0\\x1c\\x8f\\xed31\\xdb\\x92\\x87\\x16\\xb8-\\x081C\\x80\\xdd\\x00\\x08\\x13\\x02\\x13\\x03\\x13\\x01\\x00\\xff\\x01\\x00\\x00\\x8f\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\"]\n","Bad pipe message: %s [b'#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x00\\x1e\\x00\\x1c\\x04\\x03\\x05\\x03\\x06\\x03']\n","Bad pipe message: %s [b'\\x08\\x08\\x08\\t\\x08\\n\\x08', b'\\x04\\x08\\x05\\x08\\x06\\x04\\x01\\x05\\x01\\x06']\n","Bad pipe message: %s [b'\\x18\\xdbml:x\\x133\\n\\x96e\\xa0j\\x9cy\\\\\\xde\\x03 E\\xbb\\x00>\\x10\\xa9\\xef\\xe1\\xc1<\\xa9\\xd3\\xb3\\xbd\\xcdMl\\xd0\\x8b\\t\\xd3f\\x9e\\x96\\x83\\x1d\\xdfA\\x15Z\\x96\\xe8\\x00\\x08\\x13\\x02\\x13\\x03\\x13\\x01\\x00\\xff\\x01\\x00\\x00\\x8f\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127', b'.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x00\\x1e\\x00']\n","Bad pipe message: %s [b\"\\x99\\x9d\\x0b\\xfcW\\x81v\\xc03\\xeapT\\xab\\x07\\x98\\x08\\xfc-\\x00\\x00\\xa6\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(\\x00k\\x00j\\xc0s\\xc0w\\x00\\xc4\\x00\\xc3\\xc0#\\xc0'\\x00g\\x00@\\xc0r\\xc0v\\x00\\xbe\\x00\\xbd\\xc0\\n\\xc0\\x14\\x009\\x008\\x00\\x88\\x00\\x87\\xc0\\t\\xc0\\x13\\x003\\x002\\x00\\x9a\\x00\\x99\\x00E\\x00D\\xc0\\x07\\xc0\\x11\\xc0\\x08\\xc0\\x12\\x00\\x16\\x00\\x13\\x00\\x9d\\xc0\\xa1\\xc0\\x9d\\xc0Q\\x00\\x9c\\xc0\\xa0\\xc0\\x9c\\xc0P\\x00=\\x00\\xc0\\x00<\\x00\\xba\\x005\\x00\\x84\\x00/\\x00\\x96\\x00A\\x00\\x05\\x00\\n\\x00\\xff\"]\n","Bad pipe message: %s [b'']\n","Bad pipe message: %s [b'\\x03\\x05\\x03\\x06\\x03\\x08\\x07\\x08\\x08\\x08\\t\\x08\\n\\x08\\x0b\\x08\\x04\\x08\\x05\\x08\\x06\\x04\\x01\\x05\\x01\\x06\\x01']\n","Bad pipe message: %s [b'']\n","Bad pipe message: %s [b\"\\x03\\x02\\x03\\x04\\x00-\\x00\\x02\\x01\\x01\\x003\\x00&\\x00$\\x00\\x1d\\x00 \\xa5\\x9a?\\xb4\\xda46\\x92\\xea\\xc4\\xd0RP\\xf1R\\x8e\\xa6\\rn'<\\xae\"]\n","Bad pipe message: %s [b'\\x19\\xbd\\x15\\xf1\\xe6`*\\xe2\\xe4\\x7f\\xc9q\\xee\\x87\\xf5&\\x1c\\x99\\x00\\x00']\n","Bad pipe message: %s [b'\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\xc0\\x0f\\xc0\\x05\\x005\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\x00\\xff']\n","Bad pipe message: %s [b'H~\\xe9(\\xa3\\x8a\\x94\\xdc}4io\\x1bJ\\xf1\\xd5\\xa4\\xd7\\x00\\x00>\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\xc0\\x0f\\xc0\\x05\\x005\\xc0\\x13\\xc0']\n","Bad pipe message: %s [b'3\\x002\\x001\\x000\\xc0']\n","Bad pipe message: %s [b'\\x04\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96']\n","Bad pipe message: %s [b'\\x1f\\x05E\\x95L+\\x96>\\xb17\\xeb\\xe7\\x8d\\xf9NPp#\\x00\\x00>\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\xc0\\x0f\\xc0\\x05\\x00', b'\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\x00\\xff\\x02\\x01\\x00\\x15\\x03\\x00\\x00\\x02\\x02']\n"]}],"source":["import numpy as np\n","import pandas as pd\n","import numpy as np\n","\n","# 读取 CSV 文件\n","Q_df = pd.read_csv('data/matrix_Q_2.csv', header=None)\n","E_df = pd.read_csv('data/matrix_E_2.csv', header=None)\n","\n","# 将 pandas DataFrame 转换为 numpy 数组\n","Q = Q_df.drop(Q_df.columns[0], axis=1).values\n","E = E_df.drop(E_df.columns[0], axis=1).values\n","\n","# 设置参数\n","lambda_value = 1\n","learning_rate = 0.01\n","max_iterations = 1000\n","tolerance = 1e-6\n","\n","def gradient_descent(Q, E, lambda_value, learning_rate, max_iterations, tolerance):\n","    # 初始化权重 w1 和 w2\n","    w1 = 0.5\n","    w2 = 0.5\n","    n = len(Q)\n","\n","    for iteration in range(1, max_iterations + 1):\n","        # 计算综合评分和平均评分\n","        Score = w1 * Q + w2 * E\n","        mean_Score = np.mean(Score)\n","\n","        # 计算偏导数\n","        dObjective_dw1 = np.sum(2 * (Score - mean_Score) * (Q - np.mean(Q))) / n - lambda_value\n","        dObjective_dw2 = np.sum(2 * (Score - mean_Score) * (E - np.mean(E))) / n\n","\n","        # 判断是否满足停止条件\n","        if abs(dObjective_dw1) < tolerance and abs(dObjective_dw2) < tolerance:\n","            break\n","\n","        # 更新权重\n","        w1 = w1 - learning_rate * dObjective_dw1\n","        w2 = w2 - learning_rate * dObjective_dw2\n","\n","        # 归一化权重\n","        w_sum = w1 + w2\n","        w1 = w1 / w_sum\n","        w2 = w2 / w_sum\n","\n","    return w1, w2\n","\n","# 优化权重\n","w1, w2 = gradient_descent(Q, E, lambda_value, learning_rate, max_iterations, tolerance)\n","\n","# 输出优化后的权重\n","print(\"Optimized weights:\")\n","print(\"w1:\", w1)\n","print(\"w2:\", w2)\n","\n","P = w1 * Q + w2 * E\n","df = pd.DataFrame(P)\n","df.insert(0, \"Worker\", Q_df.iloc[:,0:1], True)\n","df"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# 18. Merge data trust and bid trust "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-16T12:10:59.207232Z","iopub.status.busy":"2023-04-16T12:10:59.206758Z","iopub.status.idle":"2023-04-16T12:10:59.245694Z","shell.execute_reply":"2023-04-16T12:10:59.244357Z","shell.execute_reply.started":"2023-04-16T12:10:59.207182Z"},"trusted":true},"outputs":[],"source":["df2 = pd.DataFrame(data_a.iloc[:, [0, -1]])\n","# 使用字典列表的值创建一个新的字典\n","data_dict = {k: v for d in reliable_worker for k, v in d.items()}\n","# 将新字典转换为Pandas DataFrame\n","df = pd.DataFrame(data_dict, index=[0])\n","df = df.T.reset_index().rename(columns={'index': 'Unnamed: 0', 0: 'Q'})\n","df = pd.merge(df, df2,on='Unnamed: 0')\n","df = df.rename(columns={'Unnamed: 0': 'Worker', 'TQ10': 'B'})\n","\n","# 将结果赋值给df['Q']\n","df['Q'] = df['Q'] - np.random.uniform(0, 0.1, size=len(df))\n","\n","trust_all = df.copy()\n","df"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# 19. Compute weights"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-16T12:10:59.247676Z","iopub.status.busy":"2023-04-16T12:10:59.247278Z","iopub.status.idle":"2023-04-16T12:10:59.258994Z","shell.execute_reply":"2023-04-16T12:10:59.257921Z","shell.execute_reply.started":"2023-04-16T12:10:59.247639Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","# 数据质量\n","quality = np.array(trust_all['Q'].tolist())\n","# 成本数据\n","cost = np.array(trust_all['B'].tolist())\n","# 将数据组合成一个矩阵\n","data_matrix = np.vstack((quality, cost))\n","# 计算熵权法权重\n","def entropy_weight(matrix):\n","    # 归一化处理\n","    normalized_matrix = matrix / matrix.sum(axis=1)[:, np.newaxis]\n","    # 计算每个指标的熵值\n","    entropy = -np.nansum(normalized_matrix * np.log(normalized_matrix), axis=1) / np.log(len(matrix[0]))\n","    # 计算差异熵\n","    g = 1 - entropy\n","    # 计算权重\n","    weights = g / g.sum()\n","    return weights\n","# 计算权重\n","weights = entropy_weight(data_matrix)\n","# 提取质量权重Q和成本权重C\n","Q, B = weights\n","# 输出结果\n","print(\"Quality weight (Q):\", Q)\n","print(\"Cost weight (C):\",B)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# 20. Compute CV"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-16T12:10:59.261439Z","iopub.status.busy":"2023-04-16T12:10:59.260717Z","iopub.status.idle":"2023-04-16T12:10:59.293623Z","shell.execute_reply":"2023-04-16T12:10:59.292262Z","shell.execute_reply.started":"2023-04-16T12:10:59.261399Z"},"trusted":true},"outputs":[],"source":["#█(Cv(w_i^r )=ρ_1 )〖×T〗_i^d (w_i^r )+ρ_2 〖×T〗_i^b (w_i^r ).\n","weights = {'Q': Q, 'B': B}\n","\n","df.assign(**{col: df[col] * weight for col, weight in weights.items()})\n","df['CV'] = df['Q'] + df['B']\n","df['CV_normalized'] = (df['CV'] - df['CV'].min()) / (df['CV'].max() - df['CV'].min())\n","df = df.sort_values(by='CV_normalized', ascending=False)\n","df"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-16T12:10:59.295978Z","iopub.status.busy":"2023-04-16T12:10:59.295537Z","iopub.status.idle":"2023-04-16T12:11:00.276394Z","shell.execute_reply":"2023-04-16T12:11:00.274849Z","shell.execute_reply.started":"2023-04-16T12:10:59.295885Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","# 输入数据\n","B = df['B']\n","CV = df['CV_normalized'] # 将 contributions 和 deviations 的位置互换\n","Q = df['Q']\n","# 绘制散点图，设置颜色为价格，使用coolwarm颜色映射\n","plt.scatter(Q,CV , c=B, cmap='viridis', alpha=0.5,s=60)\n","# 添加颜色条及价格标题\n","cbar = plt.colorbar()\n","cbar.ax.set_ylabel('Bid trust',fontsize=14)\n","cbar.ax.tick_params(labelsize=14)\n","    \n","# 添加坐标轴标签和标题\n","plt.xlabel('Data trust',fontsize=14) # 将 'CV' 改为 'Data trust'\n","plt.ylabel('CV',fontsize=14) # 将 'Q' 改为 'Contribution'\n","\n","# 添加文本和边框\n","plt.text(0.05, 0.85, '$\\\\rho_2=%s$' % (round(weights['B'],3)), transform=plt.gca().transAxes, fontsize=14,\n","         verticalalignment='top', bbox=dict(facecolor='white', edgecolor='white', pad=5.0))\n","plt.text(0.05, 0.93, '$\\\\rho_1=%s$' % (round(weights['Q'],3)), transform=plt.gca().transAxes, fontsize=14,\n","         verticalalignment='top', bbox=dict(facecolor='white', edgecolor='white', pad=5.0))\n","plt.xticks(size=14)\n","plt.yticks(size=14)\n","# 显示图形\n","plt.savefig('CV-1.svg', format='svg',bbox_inches='tight')\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-16T12:11:00.278921Z","iopub.status.busy":"2023-04-16T12:11:00.278381Z","iopub.status.idle":"2023-04-16T12:11:06.811433Z","shell.execute_reply":"2023-04-16T12:11:06.810450Z","shell.execute_reply.started":"2023-04-16T12:11:00.278867Z"},"trusted":true},"outputs":[],"source":["## import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn.svm import OneClassSVM\n","from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n","GT_FILE = '/kaggle/input/dbdtd-1/gt_data_2.csv'\n","UN_FILE = '/kaggle/input/dbdtd-1/unidentified_2.csv'\n","# 加载 CSV 文件\n","data = pd.read_csv(GT_FILE)\n","unidentified_data = pd.read_csv(UN_FILE)\n","labels = unidentified_data['Label'].values\n","unidentified_data = unidentified_data.drop(columns=['Label'])\n","n_columns = data.shape[1]\n","n_columns_1 = unidentified_data.shape[1]\n","lambdas=0.5\n","# 每两列进行异常检测\n","for i in range(1, n_columns, 2):\n","    X = data.iloc[:, i:i + 2].values\n","    clf = OneClassSVM(nu=lambdas, kernel=\"rbf\", gamma=0.01)\n","    clf.fit(X)\n","    X_2 = unidentified_data.iloc[:, i:i + 2].values\n","    y_pred = clf.predict(X_2)\n","    # 计算训练数据和预测数据的最小值和最大值\n","    x_min = min(X[:, 0].min(), X_2[:, 0].min()) - 1\n","    x_max = max(X[:, 0].max(), X_2[:, 0].max()) + 1\n","    y_min = min(X[:, 1].min(), X_2[:, 1].min()) - 1\n","    y_max = max(X[:, 1].max(), X_2[:, 1].max()) + 1\n","    # 在这个范围内创建网格\n","    xx, yy = np.meshgrid(np.linspace(x_min, x_max+10, 100), np.linspace(y_min, y_max+10, 100))\n","    Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n","    Z = Z.reshape(xx.shape)\n","    plt.figure()\n","    plt.contourf(xx, yy, Z, levels=np.linspace(Z.min(), 0, 15), cmap=plt.cm.Blues)\n","    plt.contour(xx, yy, Z, levels=[0], linewidths=1, colors='red')\n","    plt.contourf(xx, yy, Z, levels=[0, Z.max()], colors='palevioletred')  \n","    a = plt.scatter(X[:, 0], X[:, 1], c='white', edgecolors='k', marker='o', s=60, label='Ground truth data')\n","    b = plt.scatter(X_2[y_pred == 1, 0], X_2[y_pred == 1, 1], c='#D00000', edgecolors='k', marker='^', s=60, label='Reliable data')\n","    c = plt.scatter(X_2[y_pred == -1, 0], X_2[y_pred == -1, 1], c='#FFBA08', edgecolors='k', marker='x', s=60, label='Unreliable data')\n","    plt.text(10,30, r'$\\lambda=%s$' % (lambdas),fontsize=14)\n","    plt.legend()\n","    plt.xticks(size=14)\n","    plt.yticks(size=14)\n","    plt.legend([a, b, c],[ \"Ground truth data\",\"Reliable data\", \"Unreliable data\"], prop={'size': 14})\n","    plt.savefig(f'scatterplot-contour{i}.svg', format='svg',bbox_inches='tight')\n","    plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-16T12:11:06.813717Z","iopub.status.busy":"2023-04-16T12:11:06.812698Z","iopub.status.idle":"2023-04-16T12:11:07.037797Z","shell.execute_reply":"2023-04-16T12:11:07.036214Z","shell.execute_reply.started":"2023-04-16T12:11:06.813667Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from scipy.stats import t, sem\n","\n","# 生成示例数据\n","np.random.seed(0)\n","x = np.random.rand(50)\n","y = 2 * x + np.random.normal(0, 0.1, size=len(x))\n","\n","# 计算均值、标准误差和置信区间\n","mean_x, mean_y = np.mean(x), np.mean(y)\n","se_x, se_y = sem(x), sem(y)\n","ci_x = t.interval(0.95, len(x) - 1, loc=mean_x, scale=se_x)\n","ci_y = t.interval(0.95, len(y) - 1, loc=mean_y, scale=se_y)\n","\n","# 绘制散点图和置信区间\n","plt.scatter(x, y, label=\"数据点\")\n","plt.axvline(x=ci_x[0], color=\"red\", linestyle=\"--\", label=\"置信区间\")\n","plt.axvline(x=ci_x[1], color=\"red\", linestyle=\"--\")\n","plt.axhline(y=ci_y[0], color=\"green\", linestyle=\"--\", label=\"置信区间\")\n","plt.axhline(y=ci_y[1], color=\"green\", linestyle=\"--\")\n","\n","plt.xlabel(\"x轴\")\n","plt.ylabel(\"y轴\")\n","plt.legend()\n","plt.title(\"落在置信区间内的散点图\")\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-16T12:11:07.041507Z","iopub.status.busy":"2023-04-16T12:11:07.039848Z","iopub.status.idle":"2023-04-16T12:11:07.254859Z","shell.execute_reply":"2023-04-16T12:11:07.253809Z","shell.execute_reply.started":"2023-04-16T12:11:07.041460Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from scipy.stats import t, sem\n","# 读取数据\n","\n","# 假设的数据集，请替换为您的实际数据集\n","data = np.random.randint(1, 100, size=(100, 10))\n","bid = pd.DataFrame(data, columns=[f\"任务{i+1}\" for i in range(10)])\n","\n","# 计算均值、标准误差和置信区间\n","mean_bid = bid.mean()\n","se_bid = bid.sem()\n","ci_lower = []\n","ci_upper = []\n","\n","for i in range(10):\n","    ci = t.interval(0.95, len(bid) - 1, loc=mean_bid[i], scale=se_bid[i])\n","    ci_lower.append(ci[0])\n","    ci_upper.append(ci[1])\n","\n","ci_lower = np.array(ci_lower)\n","ci_upper = np.array(ci_upper)\n","\n","# 绘制柱状图和置信区间\n","fig, ax = plt.subplots()\n","x_labels = [f\"任务{i + 1}\" for i in range(10)]\n","x = np.arange(len(x_labels))\n","width = 0.4\n","\n","rects = ax.bar(x, mean_bid, width, yerr=(ci_upper - ci_lower) / 2, capsize=5, label=\"111\")\n","ax.set_xticks(x)\n","ax.set_xticklabels(x_labels)\n","ax.set_ylabel(\"报价\")\n","ax.set_title(\"各任务报价柱状图及置信区间\")\n","ax.legend()\n","\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-16T12:11:07.256736Z","iopub.status.busy":"2023-04-16T12:11:07.256378Z","iopub.status.idle":"2023-04-16T12:11:07.277241Z","shell.execute_reply":"2023-04-16T12:11:07.275800Z","shell.execute_reply.started":"2023-04-16T12:11:07.256703Z"},"trusted":true},"outputs":[],"source":["\n","bid = pd.read_csv(BID_FILE)\n","selected_data = bid.iloc[df.iloc[0:9,].index].drop('Unnamed: 0',axis=1).iloc[0:9,:-1]\n","# 只取对角线上的数据，其他数据设置为0\n","diagonal_data = np.zeros(selected_data.shape)\n","np.fill_diagonal(diagonal_data, selected_data.values.diagonal())\n","# 将结果转换为pandas DataFrame\n","diagonal_data_df = pd.DataFrame(diagonal_data, index=selected_data.index, columns=selected_data.columns)\n","after_detection = diagonal_data_df.sum()\n","for j in range(len(OCSVM_cost)):\n","    l = j+1\n","    if l > 1:\n","        OCSVM_cost[j] = after_detection[j-1]\n","combined_list = [MTI_cost,MVI_cost,WTI_cost,OCSVM_cost]\n","matrix_cost = np.array(combined_list)\n","min_val = np.min(matrix_cost.T)\n","max_val = np.max(matrix_cost.T)\n","normalized_matrix_cost = (matrix_cost.T - min_val) / (max_val - min_val)\n","print(normalized_matrix_cost)            "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-16T12:40:28.593431Z","iopub.status.busy":"2023-04-16T12:40:28.592975Z","iopub.status.idle":"2023-04-16T12:40:28.847321Z","shell.execute_reply":"2023-04-16T12:40:28.846322Z","shell.execute_reply.started":"2023-04-16T12:40:28.593391Z"},"trusted":true},"outputs":[],"source":["\n","import pandas as pd\n","import seaborn as sns\n","data = np.array(normalized_matrix_cost)\n","\n","\n","# 将每一列作为一个特征\n","feature1 = data[:, 0]\n","feature2 = data[:, 1]\n","feature3 = data[:, 2]\n","feature4 = data[:, 3]\n","\n","# 绘制折线图\n","fig, ax = plt.subplots(figsize=(8, 6))\n","ax.plot(feature1, label='Feature 1')\n","ax.plot(feature2, label='Feature 2')\n","ax.plot(feature3, label='Feature 3')\n","ax.plot(feature4, label='Feature 4')\n","ax.legend()\n","ax.set_xlabel('Samples')\n","ax.set_ylabel('Feature Values')\n","ax.set_title('Line Plot of Features')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-04-16T12:11:07.691712Z","iopub.status.idle":"2023-04-16T12:11:07.692149Z","shell.execute_reply":"2023-04-16T12:11:07.691950Z","shell.execute_reply.started":"2023-04-16T12:11:07.691928Z"},"trusted":true},"outputs":[],"source":["array([85.48538775, 82.89677339, 89.12708174, 87.00855861, 84.90491896,\n","       88.7948461 , 91.38832448, 75.20164938, 85.45880388])"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"}},"nbformat":4,"nbformat_minor":4}
