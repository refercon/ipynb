{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10355524,"sourceType":"datasetVersion","datasetId":6412963},{"sourceId":10355855,"sourceType":"datasetVersion","datasetId":6413216},{"sourceId":10356145,"sourceType":"datasetVersion","datasetId":6413450}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\n\nneg=pd.read_csv('/kaggle/input/datasetsandmodel/neg.csv',header=None,index_col=None)\npos=pd.read_csv('/kaggle/input/datasetsandmodel/pos.csv',header=None,index_col=None,on_bad_lines='skip')\nneu=pd.read_csv('/kaggle/input/datasetsandmodel/neutral.csv', header=None, index_col=None)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T01:52:27.674325Z","iopub.execute_input":"2025-01-03T01:52:27.674740Z","iopub.status.idle":"2025-01-03T01:52:28.459306Z","shell.execute_reply.started":"2025-01-03T01:52:27.674683Z","shell.execute_reply":"2025-01-03T01:52:28.457417Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import numpy as np\n\ncombined = np.concatenate((pos[0], neu[0], neg[0]))\ncombined.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T01:52:28.460972Z","iopub.execute_input":"2025-01-03T01:52:28.461373Z","iopub.status.idle":"2025-01-03T01:52:28.477654Z","shell.execute_reply.started":"2025-01-03T01:52:28.461341Z","shell.execute_reply":"2025-01-03T01:52:28.476629Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"(21088,)"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"# pos -> 1; neu -> 0; neg -> -1\ny = np.concatenate((np.ones(len(pos), dtype=int), np.zeros(len(neu), dtype=int), -1*np.ones(len(neg),dtype=int)))\ny.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T02:28:15.422222Z","iopub.execute_input":"2025-01-03T02:28:15.422613Z","iopub.status.idle":"2025-01-03T02:28:15.429923Z","shell.execute_reply.started":"2025-01-03T02:28:15.422582Z","shell.execute_reply":"2025-01-03T02:28:15.429060Z"}},"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"(21088,)"},"metadata":{}}],"execution_count":27},{"cell_type":"code","source":"import jieba\n\n# 对句子进行分词，并去掉换行符\ndef tokenizer(text):\n    ''' Simple Parser converting each document to lower-case, then\n        removing the breaks for new lines and finally splitting on the\n        whitespace\n    '''\n    text = [jieba.lcut(str(document).replace('\\n', '')) for document in text]  # Ensure document is a string\n    return text\n\ncombined = tokenizer(combined)\nlen(combined)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T01:52:28.478680Z","iopub.execute_input":"2025-01-03T01:52:28.479038Z","iopub.status.idle":"2025-01-03T01:52:40.720552Z","shell.execute_reply.started":"2025-01-03T01:52:28.479008Z","shell.execute_reply":"2025-01-03T01:52:40.719391Z"}},"outputs":[{"name":"stderr","text":"Building prefix dict from the default dictionary ...\nDumping model to file cache /tmp/jieba.cache\nLoading model cost 0.963 seconds.\nPrefix dict has been built successfully.\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"21088"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"from gensim.models.word2vec import Word2Vec\nfrom gensim.corpora.dictionary import Dictionary\nfrom keras.preprocessing import sequence\nimport multiprocessing\n\ncpu_count = multiprocessing.cpu_count() # 4\nvocab_dim = 100\nn_iterations = 10  # ideally more..\nn_exposures = 10 # 所有频数超过10的词语\nwindow_size = 7\nn_epoch = 4\ninput_length = 100\nmaxlen = 100\n\ndef create_dictionaries(model=None,\n                        combined=None):\n    ''' Function does are number of Jobs:\n        1- Creates a word to index mapping\n        2- Creates a word to vector mapping\n        3- Transforms the Training and Testing Dictionaries\n    '''\n    if (combined is not None) and (model is not None):\n        gensim_dict = Dictionary()\n        gensim_dict.doc2bow(model.wv.index_to_key,\n                            allow_update=True)\n        #  freqxiao10->0 所以k+1\n        w2indx = {v: k+1 for k, v in gensim_dict.items()} # 所有频数超过10的词语的索引, (k->v)=>(v->k)\n        w2vec = {word: model.wv[word] for word in w2indx.keys()}  # 所有频数超过10的词语的词向量, (word->model(word))\n\n        def parse_dataset(combined):  # 闭包-->临时使用\n            ''' Words become integers '''\n            data = []\n            for sentence in combined:\n                new_txt = []\n                for word in sentence:\n                    try:\n                        new_txt.append(w2indx[word])\n                    except:\n                        new_txt.append(0)  # freqxiao10->0\n                data.append(new_txt)\n            return data  # word=>index\n        combined = parse_dataset(combined)\n        combined = sequence.pad_sequences(combined, maxlen=maxlen)  # 每个句子所含词语对应的索引，所以句子中含有频数小于10的词语，索引为0\n        return w2indx, w2vec, combined\n    else:\n        print('No data provided...')\n\n\n# 创建词语字典，并返回每个词语的索引，词向量，以及每个句子所对应的词语索引\ndef word2vec_train(combined):\n    model = Word2Vec(vector_size=vocab_dim,  # Changed 'size' to 'vector_size'\n                     min_count=n_exposures,\n                     window=window_size,\n                     workers=cpu_count,\n                     epochs=n_iterations)  # 'iter' replaced with 'epochs'\n    model.build_vocab(combined)  # input: list\n    model.train(combined, total_examples=model.corpus_count, epochs=model.epochs)  # Update the training call\n    model.save('Word2vec_model.pkl')\n    index_dict, word_vectors, combined = create_dictionaries(model=model, combined=combined)\n    return index_dict, word_vectors, combined\n\nprint('Training a Word2vec model...')\nindex_dict, word_vectors, combined = word2vec_train(combined)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T02:27:26.036244Z","iopub.execute_input":"2025-01-03T02:27:26.036631Z","iopub.status.idle":"2025-01-03T02:27:36.690701Z","shell.execute_reply.started":"2025-01-03T02:27:26.036597Z","shell.execute_reply":"2025-01-03T02:27:36.689665Z"}},"outputs":[{"name":"stdout","text":"Training a Word2vec model...\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom tensorflow.keras.models import Sequential, load_model  # Updated import\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Activation  # Updated import\nimport numpy as np\nimport keras\nimport sys\nn_epoch = 10\nnp.random.seed(1337)  # For Reproducibility\nsys.setrecursionlimit(1000000)\n\nbatch_size = 32\n\ndef get_data(index_dict, word_vectors, combined, y):\n    n_symbols = len(index_dict) + 1  # 所有单词的索引数，频数小于10的词语索引为0，所以加1\n    embedding_weights = np.zeros((n_symbols, vocab_dim))  # 初始化 索引为0的词语，词向量全为0\n    for word, index in index_dict.items():  # 从索引为1的词语开始，对每个词语对应其词向量\n        embedding_weights[index, :] = word_vectors[word]\n    x_train, x_test, y_train, y_test = train_test_split(combined, y, test_size=0.2)\n    y_train = keras.utils.to_categorical(y_train, num_classes=3) \n    y_test = keras.utils.to_categorical(y_test, num_classes=3)\n    return n_symbols, embedding_weights, x_train, y_train, x_test, y_test\n\n# Define F1 score function\n\nfrom tensorflow.keras import backend as K\n\ndef f1_score(y_true, y_pred):\n    # 获取类别数\n    num_classes = K.int_shape(y_true)[-1]\n\n    # 将 y_pred 和 y_true 转换为类别标签\n    y_pred = K.argmax(y_pred, axis=-1)\n    y_true = K.argmax(y_true, axis=-1)\n\n    # 初始化 F1-score\n    f1 = 0.0\n\n    # 对每个类别计算 F1-score\n    for i in range(num_classes):\n        # 计算该类别的 true positives, predicted positives 和 possible positives\n        true_positives = K.sum(K.cast(K.equal(y_true, i) & K.equal(y_pred, i), 'float32'))\n        predicted_positives = K.sum(K.cast(K.equal(y_pred, i), 'float32'))\n        possible_positives = K.sum(K.cast(K.equal(y_true, i), 'float32'))\n\n        # 计算该类别的 precision 和 recall\n        precision = true_positives / (predicted_positives + K.epsilon())\n        recall = true_positives / (possible_positives + K.epsilon())\n\n        # 计算该类别的 F1-score\n        f1_class = 2 * (precision * recall) / (precision + recall + K.epsilon())\n        \n        # 将该类别的 F1-score 加入总的 F1-score\n        f1 += f1_class\n\n    # 计算平均 F1-score\n    f1 = f1 / num_classes\n    return f1\n# 定义网络结构\ndef train_lstm(n_symbols, embedding_weights, x_train, y_train, x_test, y_test):\n    print('Defining a Simple Keras Model...')\n    model = Sequential()  # or Graph or whatever\n    model.add(Embedding(output_dim=vocab_dim,\n                        input_dim=n_symbols,\n                        mask_zero=True,\n                        weights=[embedding_weights],\n                        input_length=input_length))  # Adding Input Length\n    model.add(LSTM(units=50, activation='tanh', recurrent_activation='hard_sigmoid'))  # Updated LSTM\n    model.add(Dropout(0.5))\n    model.add(Dense(3, activation='softmax'))  # Dense=>全连接层, 输出维度=3\n    model.add(Activation('softmax'))\n\n    print('Compiling the Model...')\n    model.compile(loss='categorical_crossentropy',\n                  optimizer='adam', metrics=['accuracy',f1_score])\n\n    print(\"Train...\")  # batch_size=32\n    model.fit(x_train, y_train, batch_size=batch_size, epochs=n_epoch, verbose=1)\n\n    print(\"Evaluate...\")\n    score = model.evaluate(x_test, y_test, batch_size=batch_size)\n\n    # Save the model directly (no YAML needed)\n    model.save('lstm_model.h5')  # Save the whole model\n    print('Test score:', score)\n\n# If you need to load the model later, use:\n# model = load_model('../model/lstm_model.h5')\n\nprint('Setting up Arrays for Keras Embedding Layer...')\nn_symbols, embedding_weights, x_train, y_train, x_test, y_test = get_data(index_dict, word_vectors, combined, y)\nprint(\"x_train.shape and y_train.shape:\")\nprint(x_train.shape, y_train.shape)\ntrain_lstm(n_symbols, embedding_weights, x_train, y_train, x_test, y_test)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T03:02:54.229713Z","iopub.execute_input":"2025-01-03T03:02:54.230197Z","iopub.status.idle":"2025-01-03T03:08:08.541671Z","shell.execute_reply.started":"2025-01-03T03:02:54.230153Z","shell.execute_reply":"2025-01-03T03:08:08.540392Z"}},"outputs":[{"name":"stdout","text":"Setting up Arrays for Keras Embedding Layer...\nx_train.shape and y_train.shape:\n(16870, 100) (16870, 3)\nDefining a Simple Keras Model...\nCompiling the Model...\nTrain...\nEpoch 1/10\n\u001b[1m528/528\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 59ms/step - accuracy: 0.5453 - f1_score: 0.4119 - loss: 0.9786\nEpoch 2/10\n\u001b[1m528/528\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 58ms/step - accuracy: 0.7443 - f1_score: 0.6812 - loss: 0.8058\nEpoch 3/10\n\u001b[1m528/528\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 58ms/step - accuracy: 0.8512 - f1_score: 0.8545 - loss: 0.7019\nEpoch 4/10\n\u001b[1m528/528\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 58ms/step - accuracy: 0.8899 - f1_score: 0.8890 - loss: 0.6631\nEpoch 5/10\n\u001b[1m528/528\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 58ms/step - accuracy: 0.8724 - f1_score: 0.8767 - loss: 0.6810\nEpoch 6/10\n\u001b[1m528/528\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 58ms/step - accuracy: 0.8960 - f1_score: 0.8979 - loss: 0.6557\nEpoch 7/10\n\u001b[1m528/528\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 58ms/step - accuracy: 0.9009 - f1_score: 0.9009 - loss: 0.6518\nEpoch 8/10\n\u001b[1m528/528\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 58ms/step - accuracy: 0.9158 - f1_score: 0.9137 - loss: 0.6354\nEpoch 9/10\n\u001b[1m528/528\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 59ms/step - accuracy: 0.9191 - f1_score: 0.9170 - loss: 0.6327\nEpoch 10/10\n\u001b[1m528/528\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 59ms/step - accuracy: 0.9221 - f1_score: 0.9200 - loss: 0.6287\nEvaluate...\n\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - accuracy: 0.8928 - f1_score: 0.8933 - loss: 0.6582\nTest score: [0.6595293879508972, 0.8914177417755127, 0.8914803266525269]\n","output_type":"stream"}],"execution_count":58},{"cell_type":"code","source":"import jieba\nimport numpy as np\nfrom gensim.models.word2vec import Word2Vec\nfrom gensim.corpora.dictionary import Dictionary\nfrom keras.preprocessing import sequence\nfrom tensorflow.keras.models import load_model  # Updated import\n\nnp.random.seed(1337)  # For Reproducibility\nimport sys\nsys.setrecursionlimit(1000000)\n\n# Define parameters\nmaxlen = 100\n\ndef create_dictionaries(model=None, combined=None):\n    ''' Function does a number of jobs:\n        1- Creates a word to index mapping\n        2- Creates a word to vector mapping\n        3- Transforms the Training and Testing Dictionaries\n    '''\n    if (combined is not None) and (model is not None):\n        gensim_dict = Dictionary()\n        # Use model.wv instead of model.vocab\n        gensim_dict.doc2bow(model.wv.index_to_key, allow_update=True)  # Updated line\n        # freqxiao10->0 so k+1\n        w2indx = {v: k+1 for k, v in gensim_dict.items()}  # All frequent words indexed\n        w2vec = {word: model.wv[word] for word in w2indx.keys()}  # Word vectors for frequent words\n\n        def parse_dataset(combined):  # closure for temporary use\n            ''' Words become integers '''\n            data = []\n            for sentence in combined:\n                new_txt = []\n                for word in sentence:\n                    try:\n                        new_txt.append(w2indx[word])\n                    except:\n                        new_txt.append(0)  # freqxiao10->0\n                data.append(new_txt)\n            return data  # Word => Index\n        combined = parse_dataset(combined)\n        combined = sequence.pad_sequences(combined, maxlen=maxlen)  # Pad sequences\n        return w2indx, w2vec, combined\n    else:\n        print('No data provided...')\n\n\ndef input_transform(string):\n    words = jieba.lcut(string)\n    words = np.array(words).reshape(1, -1)\n    model = Word2Vec.load('/kaggle/working/Word2vec_model.pkl')\n    _, _, combined = create_dictionaries(model, words)\n    return combined\n\n\ndef lstm_predict(string):\n    #print('Loading model......')\n    model = load_model('/kaggle/working/lstm_model.h5')  # Load the model\n\n    #print('Preparing input data......')\n    data = input_transform(string)\n    data = data.reshape(1, -1)\n\n    #print('Predicting...')\n    result = model.predict(data)  # Make prediction\n    predicted_class = np.argmax(result, axis=1)  # Get the class with the highest probability\n\n    # print(result)  # Display the prediction probabilities\n    if predicted_class[0] == 1:\n        print(string, 'positive')\n    elif predicted_class[0] == 0:\n        print(string, 'neutral')\n    else:\n        print(string, 'negative')\n    return predicted_class[0]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T02:38:45.016063Z","iopub.execute_input":"2025-01-03T02:38:45.016581Z","iopub.status.idle":"2025-01-03T02:38:45.039503Z","shell.execute_reply.started":"2025-01-03T02:38:45.016534Z","shell.execute_reply":"2025-01-03T02:38:45.037777Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"string = \"但是内容实在没意思\"\n# string = \"真的一般，没什么可以学习的\"\n\nlstm_predict(string)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T03:39:27.629711Z","iopub.execute_input":"2025-01-03T03:39:27.630308Z","iopub.status.idle":"2025-01-03T03:39:28.201687Z","shell.execute_reply.started":"2025-01-03T03:39:27.630270Z","shell.execute_reply":"2025-01-03T03:39:28.200622Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 254ms/step\n但是内容实在没意思 neutral\n","output_type":"stream"},{"execution_count":59,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}],"execution_count":59},{"cell_type":"code","source":"# string='酒店的环境非常好，价格也便宜，值得推荐'\n# string='手机质量太差了，傻逼店家，赚黑心钱，以后再也不会买了'\n# string = \"这是我看过文字写得很糟糕的书，因为买了，还是耐着性子看完了，但是总体来说不好，文字、内容、结构都不好\"\n# string = \"虽说是职场指导书，但是写的有点干涩，我读一半就看不下去了！\"\n# string = \"书的质量还好，但是内容实在没意思。本以为会侧重心理方面的分析，但实际上是婚外恋内容。\"\n# string = \"不是太好\"\n# string = \"不错不错\"\n\n\n\n\n# Initialize counters for each category\n# positive_count = 0\n# neutral_count = 0\n# negative_count = 0\n\n# # Load the CSV file\n# text = pd.read_csv('/kaggle/input/ddddddd/comments1.csv', header=None, index_col=None)\n\n# # Extract comments from the second column (index 1)\n# comments = text.iloc[:, 1]\n\n# # Iterate over each comment and predict using lstm_predict\n# for comment in comments:\n#     #print(f\"Predicting for comment: {comment}\")\n#     result = lstm_predict(comment)  # Assuming lstm_predict returns the prediction result\n\n#     if result == 1:\n#         positive_count += 1\n#     elif result == 0:\n#         neutral_count += 1\n#     else:\n#         negative_count += 1\n\n# # Print the statisticsprint(\"\\nClassification Statistics:\")\n# print(f\"Positive: {positive_count}\")\n# print(f\"Neutral: {neutral_count}\")\n# print(f\"Negative: {negative_count}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T02:40:45.579192Z","iopub.execute_input":"2025-01-03T02:40:45.579683Z","iopub.status.idle":"2025-01-03T02:40:46.091926Z","shell.execute_reply.started":"2025-01-03T02:40:45.579643Z","shell.execute_reply":"2025-01-03T02:40:46.090659Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step\n但是内容实在没意思 neutral\n","output_type":"stream"},{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}],"execution_count":44},{"cell_type":"code","source":"","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null}]}